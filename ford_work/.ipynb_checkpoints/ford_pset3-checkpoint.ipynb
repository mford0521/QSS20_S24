{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3: Text analysis of DOJ press releases\n",
    "\n",
    "**Total points (without extra credit)**: 52 \n",
    "\n",
    "- For background:\n",
    "\n",
    "    - DOJ is the federal law enforcement agency responsible for federal prosecutions; this contrasts with the local prosecutions in the Cook County dataset we analyzed earlier. Here's a short explainer on which crimes get prosecuted federally versus locally: https://www.criminaldefenselawyer.com/resources/criminal-defense/federal-crime/state-vs-federal-crimes.htm#:~:text=Federal%20criminal%20prosecutions%20are%20handled,of%20state%20and%20local%20law. \n",
    "    - Here's the Kaggle that contains the data: https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases \n",
    "    - Here's the code the dataset creator used to scrape those press releases here if you're interested: https://github.com/jbencina/dojreleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "### uncomment and run these lines if you haven't downloaded relevant nltk add-ons yet\n",
    "### nltk.download('averaged_perceptron_tagger')\n",
    "### nltk.download('stopwords')\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "## spacy imports\n",
    "import spacy\n",
    "### uncomment and run the below line if you haven't loaded the en_core_web_sm library yet\n",
    "### ! python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## lda\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## repeated printouts and wide-format text\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load and clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>contents</th>\n",
       "      <th>date</th>\n",
       "      <th>topics_clean</th>\n",
       "      <th>components_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Convicted Bomb Plotter Sentenced to 30 Years</td>\n",
       "      <td>PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077</td>\n",
       "      <td>2014-10-01T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>National Security Division (NSD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-919</td>\n",
       "      <td>$1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands</td>\n",
       "      <td>WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.</td>\n",
       "      <td>2012-07-25T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1002</td>\n",
       "      <td>$1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts</td>\n",
       "      <td>BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn &amp; Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn &amp; Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace &amp; Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.</td>\n",
       "      <td>2011-08-03T00:00:00-04:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-015</td>\n",
       "      <td>10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests</td>\n",
       "      <td>WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.</td>\n",
       "      <td>2010-01-08T00:00:00-05:00</td>\n",
       "      <td>No topic</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-898</td>\n",
       "      <td>$100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.</td>\n",
       "      <td>The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black &amp; Decker Inc.—Emhart Industries Inc. and Black &amp; Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black &amp; Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black &amp; Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.</td>\n",
       "      <td>2018-07-09T00:00:00-04:00</td>\n",
       "      <td>Environment</td>\n",
       "      <td>Environment and Natural Resources Division</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0     None   \n",
       "1  12-919    \n",
       "2  11-1002   \n",
       "3   10-015   \n",
       "4   18-898   \n",
       "\n",
       "                                                                                                          title  \\\n",
       "0                                                                  Convicted Bomb Plotter Sentenced to 30 Years   \n",
       "1                              $1 Million in Restitution Payments Announced to Preserve North Carolina Wetlands   \n",
       "2                 $1 Million Settlement Reached for Natural Resource Damages at Superfund Site in Massachusetts   \n",
       "3                                          10 Las Vegas Men Indicted \\r\\nfor Falsifying Vehicle Emissions Tests   \n",
       "4  $100 Million Settlement Will Speed Cleanup Work at Centredale Manor Superfund Site in North Providence, R.I.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          contents  \\\n",
       "0  PORTLAND, Oregon. – Mohamed Osman Mohamud, 23, who was convicted in 2013 of attempting to use a weapon of mass destruction (explosives) in connection with a plot to detonate a vehicle bomb at an annual Christmas tree lighting ceremony in Portland, was sentenced today to serve 30 years in prison, followed by a lifetime term of supervised release. Mohamud, a naturalized U.S. citizen from Somalia and former resident of Corvallis, Oregon, was arrested on Nov. 26, 2010, after he attempted to detonate what he believed to be an explosives-laden van that was parked near the tree lighting ceremony in Portland.  The arrest was the culmination of a long-term undercover operation, during which Mohamud was monitored closely for months as his bomb plot developed.  The device was in fact inert, and the public was never in danger from the device. At sentencing, United States District Court Judge Garr M. King, who presided over Mohamed’s 14-day trial, said “the intended crime was horrific,” and that the defendant, even though he was presented with options by undercover FBI employees, “never once expressed a change of heart.”  King further noted that the Christmas tree ceremony was attended by up to 10,000 people, and that the defendant “wanted everyone to leave either dead or injured.”  King said his sentence was necessary in view of the seriousness of the crime and to serve as deterrence to others who might consider similar acts.     “With today’s sentencing, Mohamed Osman Mohamud is being held accountable for his attempted use of what he believed to be a massive bomb to attack innocent civilians attending a public Christmas tree lighting ceremony in Portland,” said John P. Carlin, Assistant Attorney General for National Security.  “The evidence clearly indicated that Mohamud was intent on killing as many people as possible with his attack.  Fortunately, law enforcement was able to identify him as a threat, insert themselves in the place of a terrorist that Mohamud was trying to contact, and thwart Mohamud’s efforts to conduct an attack on our soil.  This case highlights how the use of undercover operations against would-be terrorists allows us to engage and disrupt those who wish to commit horrific acts of violence against the innocent public.  The many agents, analysts, and prosecutors who have worked on this case deserve great credit for their roles in protecting Portland from the threat posed by this defendant and ensuring that he was brought to justice.” “This trial provided a rare glimpse into the techniques Al Qaeda employs to radicalize home-grown extremists,” said Amanda Marshall, U.S. Attorney for the District of Oregon.  “With the sentencing today, the court has held this defendant accountable.   I thank the dedicated professionals in the law enforcement and intelligence communities who were responsible for this successful outcome.  I look forward to our continued work with Muslim communities in Oregon who are committed to ensuring that all young people are safe from extremists who seek to radicalize others to engage in violence.”  According to the trial evidence, in February 2009, Mohamud began communicating via e-mail with Samir Khan, a now-deceased al Qaeda terrorist who published Jihad Recollections, an online magazine that advocated violent jihad, and who also published Inspire, the official magazine of al-Qaeda in the Arabian Peninsula.  Between February and August 2009, Mohamed exchanged approximately 150 emails with Khan.  Mohamud wrote several articles for Jihad Recollections that were published under assumed names. In August 2009, Mohamud was in email contact with Amro Al-Ali, a Saudi national who was in Yemen at the time and is today in custody in Saudi Arabia for terrorism offenses.  Al-Ali sent Mohamud detailed e-mails designed to facilitate Mohamud’s travel to Yemen to train for violent jihad.  In December 2009, while Al-Ali was in the northwest frontier province of Pakistan, Mohamud and Al-Ali discussed the possibility of Mohamud traveling to Pakistan to join Al-Ali in terrorist activities. Mohamud responded to Al-Ali in an e-mail: “yes, that would be wonderful, just tell me what I need to do.”  Al-Ali referred Mohamud to a second associate overseas and provided Mohamud with a name and email address to facilitate the process. In the following months, Mohamud made several unsuccessful attempts to contact Al-Ali’s associate.  Ultimately, an FBI undercover operative contacted Mohamud via email under the guise of being an associate of Al-Ali’s.  Mohamud and the FBI undercover operative agreed to meet in Portland in July 2010.  At the meeting, Mohamud told the FBI undercover operative he had written articles that were published in Jihad Recollections.  Mohamud also said that he wanted to become “operational.”  Asked what he meant by “operational,” Mohamud said he wanted to put an explosion together, but needed help. According to evidence presented at trial, at a meeting in August 2010, Mohamud told undercover FBI operatives he had been thinking of committing violent jihad since the age of 15.  Mohamud then told the undercover FBI operatives that he had identified a potential target for a bomb: the annual Christmas tree lighting ceremony in Portland’s Pioneer Courthouse Square on Nov. 26, 2010.  The undercover FBI operatives cautioned Mohamud several times about the seriousness of this plan, noting there would be many people at the event, including children, and emphasized that Mohamud could abandon his attack plans at any time with no shame.  Mohamud indicated the deaths would be justified and that he would not mind carrying out a suicide attack on the crowd. According to evidence presented at trial, in the ensuing months Mohamud continued to express his interest in carrying out the attack and worked on logistics.  On Nov. 4, 2010, Mohamud and the undercover FBI operatives traveled to a remote location in Lincoln County, Oregon, where they detonated a bomb concealed in a backpack as a trial run for the upcoming attack.  During the drive back to Corvallis, Mohamud was asked if was capable looking at all the bodies of those who would be killed during the explosion.  In response, Mohamud noted, “I want whoever is attending that event to be, to leave either dead or injured.”  Mohamud later recorded a video of himself, with the assistance of the undercover FBI operatives, in which he read a statement that offered his rationale for his bomb attack.  On Nov. 18, 2010, undercover FBI operatives picked up Mohamud to travel to Portland to finalize the details of the attack.  On Nov. 26, 2010, just hours before the planned attack, Mohamud examined the 1,800 pound bomb in the van and remarked that it was “beautiful.”  Later that day, Mohamud was arrested after he attempted to remotely detonate the inert vehicle bomb rked near the Christmas tree lighting ceremony This case was investigated by the FBI, with assistance from the Oregon State Police, the Corvallis Police Department, the Lincoln County Sheriff’s Office and the Portland Police Bureau.  The prosecution was handled by Assistant U.S. Attorneys Ethan D. Knight and Pamala Holsinger from the U.S. Attorney’s Office for the District of Oregon.  Trial Attorney Jolie F. Zimmerman, from the Counterterrorism Section of the Justice Department’s National Security Division, assisted. # # # 14-1077   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       WASHINGTON – North Carolina’s Waccamaw River watershed will benefit from a $1 million restitution order from a federal court, funding environmental projects to acquire and preserve wetlands in an area damaged by illegal releases of wastewater from a corporate hog farm, announced Ignacia S. Moreno, Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division; U.S. Attorney for the Eastern District of North Carolina Thomas G. Walker; Director Greg McLeod from the North Carolina State Bureau of Investigation; and Camilla M. Herlevich, Executive Director of the North Carolina Coastal Land Trust.   Freedman Farms Inc. was sentenced in February 2012 to five years of probation and ordered to pay $1.5 million in fines, restitution and community service payments for violating the Clean Water Act when it discharged hog waste into a stream that leads to the Waccamaw River.  William B. Freedman, president of Freedman Farms, was sentenced to six months in prison to be followed by six months of home confinement.  Freedman Farms also is required to implement a comprehensive environmental compliance program and institute an annual training program.   In an order issued on April 19, 2012, the court ordered that the defendants would be responsible for restitution of $1 million in the form of five annual payments starting in January 2013, which the court will direct to the North Carolina Coastal Land Trust (NCCLT).  The NCCLT plans to use the money to acquire and conserve land along streams in the Waccamaw watershed.  The court also directed a $75,000 community service payment to the Southern Environmental Enforcement Network, an organization dedicated to environmental law enforcement training and information sharing in the region.    “The resolution of the case against Freedman Farms demonstrates the commitment of the Department of Justice to enforcing the Clean Water Act to ensure the protection of human health and the environment,” said Assistant Attorney General Moreno.  “The court-ordered restitution in this case will conserve wetlands for the benefit of the people of North Carolina.  By enforcing the nation’s environmental laws, we will continue to ensure that concentrated animal feeding operations (CAFOs) operate without threatening our drinking water, the health of our communities and the environment.”   “This office is committed to doing our part to hold accountable those who commit crimes against our environment, which can cause serious health problems to residents and damage the environment that makes North Carolina such a beautiful place to live and visit,” said U.S. Attorney Walker.   “This case shows what we can accomplish when our SBI agents work closely with their local, state and federal partners to investigate environmental crimes and hold the polluters accountable,” said Director McLeod.  “We’ll continue our efforts to fight illegal pollution that damages our water and puts the public’s health at risk.”    “The Waccamaw is unique and wild,” said Director Herlevich of the North Carolina Coastal Land Trust. “Its watershed includes some of the most extensive cypress gum swamps in the state, and its headwaters at Lake Waccamaw contain fish that are found nowhere else on Earth.  We appreciate the trust of the court and the U. S. Attorney, and we look forward to using these funds for conservation projects in a river system that is one of our top conservation priorities.”   According to evidence presented in court, in December 2007 Freedman Farms discharged hog waste into Browder’s Branch, a tributary to the Waccamaw River that flows through the White Marsh, a large wetlands complex.  Freedman Farms, located in Columbus County, N.C., is in the business of raising hogs for market, and this particular farm had some 4,800 hogs.  The hog waste was supposed to be directed to two lagoons for treatment and disposal.  Instead, hog waste was discharged from Freedman Farms directly into Browder’s Branch.    The Clean Water Act is a federal law that makes it illegal to knowingly or negligently discharge a pollutant into a water of the United States.    The Freedman case was investigated by the U.S. Environmental Protection Agency (EPA) Criminal Investigation Division, the U.S. Army Corps of Engineers and the North Carolina State Bureau of Investigation, with assistance from the EPA Science and Ecosystem Support Division.  The case was prosecuted by Assistant U.S. Attorney J. Gaston B. Williams of the Eastern District of North Carolina and Trial Attorney Mary Dee Carraway of the Environmental Crimes Section of the Justice Department’s Environment and Natural Resources Division.   The North Carolina Coastal Land Trust is celebrating its 20th anniversary of saving special lands in eastern North Carolina. The organization has protected nearly 50,000 acres of lands with scenic, recreational, historic and ecological values. North Carolina Coastal Land Trust has saved streams and wetlands that provide clean water, forests that are havens for wildlife, working farms that provide local food and nature parks that everyone can enjoy.  More information about the Coastal Land Trust is available at www.coastallandtrust.org.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        BOSTON– A $1-million settlement has been reached for natural resource damages (NRD) at the Blackburn & Union Privileges Superfund Site in Walpole, Mass., the Departments of Justice and Interior (DOI), and the Office of the Massachusetts Attorney General announced today.                The Blackburn & Union Privileges Superfund Site includes 22 acres of contaminated land and water in Walpole. The contamination resulted from the operations of various industrial facilities dating back to the 19th century that exposed the site to asbestos, arsenic, lead and other hazardous substances.                The private parties involved in the settlement include two former owners and operators of the site, W.R. Grace & Co.– Conn. and Tyco Healthcare Group LP, as well as the current owners, BIM Investment Corp. and Shaffer Realty Nominee Trust.               From about 1915 to 1936, a predecessor of W.R. Grace manufactured asbestos brake linings and clutch linings on a large portion of the property. From 1946 to about 1983, a predecessor of Tyco Healthcare operated a cotton fabric manufacturing business, which used caustic solutions, on a portion of the property.               In a 2010 settlement with U.S. Environmental Protection Agency (EPA), the four private parties agreed to perform a remedial action to clean up the site at an estimated cost of $13 million. The consent decree lodged today resolves both state and federal NRD liability claims; it requires the parties to pay $1,094,169.56 to the state and federal natural resource trustees, the Massachusetts Executive Office of Energy and Environmental Affairs (EEA) and DOI, for injuries to ecological resources including groundwater and wetlands, which provide habitat for waterfowl and wading birds, including black ducks and great blue herons.  The trustees will use the settlement funds for natural resource restoration projects in the area.               “This settlement demonstrates our commitment to recovering damages from the parties responsible for injury to natural resources, in partnership with state trustees,” said Bruce Gelber, Acting Deputy Assistant Attorney General of the Justice Department’s Environment and Natural Resources Division.               “The citizens of Walpole have had to live with the environmental impact of this contamination for many years,” Attorney General Martha Coakley said. “We are pleased that today’s agreement will not only require the responsible parties to reimburse taxpayer dollars, but will also provide funding to begin restoring or replacing the wetland and other natural resources.”                 The consent decree was lodged in the U.S. District Court for Massachusetts.     A portion of the funds, $300,000, will be distributed to the EEA-sponsored groundwater restoration projects; $575,000 will be used for ecological restoration projects jointly sponsored by EEA and the U.S. Fish and Wildlife Service (FWS).               In addition, $125,000 will go for projects jointly sponsored by EEA and FWS that achieve both ecological and groundwater restoration; $57,491.34 will be allocated for reimbursement for the FWS’s assessment costs; and $36,678.22 will be distributed as reimbursement for the commonwealth’s assessment costs.       “This settlement provides the means for a range of projects designed to compensate the public for decades of groundwater and other ecological damage at this site.  I encourage local citizens and organizations to become engaged in the public process that will take place as we solicit, take comment on, and choose these projects in the months ahead,” said Energy and Environmental Affairs Secretary Richard K. Sullivan Jr., who serves as the Commonwealth’s Natural Resources Damages trustee.       “This settlement will help restore habitat for fish and wildlife in the Neponset River watershed,” said Tom Chapman of the FWS New England Field Office. “We look forward to working with the commonwealth and local stakeholders to implement restoration.”               “More than 100 years-worth of industrial activities at this site caused major environmental contamination to the Neponset River, nearby wetlands and to groundwater below the site,” said Commissioner Kenneth Kimmell of the Massachusetts Department of Environmental Protection (MassDEP), which will staff the Trustee Council for the Commonwealth. “We will ensure that the community and the public will be active participants in the process to use these NRD funds to restore the injured natural resources.”                Under the federal Comprehensive Environmental Response, Compensation and Liability Act, EEA and DOI, acting through the FWS, are the designated state and federal natural resource Trustees for the site. The site has been listed on the EPA’s National Priorities List since 1994.        The consent decree is subject to a public comment period and court approval. A copy of the consent decree and instructions about how to submit comments is available on www.usdoj.gov/enrd/Consent_Decrees.html  .               After the consent decree is approved, EEA and FWS will develop proposed restoration plans to use the settlement funds for restoration projects. The proposed restoration plans will also be made available to the public for review and comment.                Assistant Attorney General Matthew Brock of Massachusetts Attorney General Coakley's Environmental Protection Division handled this matter.  Attorney Jennifer Davis of MassDEP, Attorney Anna Blumkin of EEA and MassDEP’s NRD Coordinator Karen Pelto also worked on this settlement.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           WASHINGTON—A federal grand jury in Las Vegas today returned indictments against 10 Nevada-certified emissions testers for falsifying vehicle emissions test reports, the Justice Department announced.   Each defendant faces one felony Clean Air Act count for falsifying reports between November 2007 and May 2009. The number of falsifications varied by defendant, with some defendants having falsified approximately 250 records, while others falsified more than double that figure. One defendant is alleged to have falsified over 700 reports.   The individuals indicted include:     Escudero resides in Pahrump, Nev. All other individuals are from Clark County, Nev.    The 10 defendants are alleged to have engaged in a practice known as \"clean scanning\" vehicles. The scheme involved entering the Vehicle Identification Number (VIN) for a vehicle that would not pass the emissions test into the computerized system, then connecting a different vehicle the testers knew would pass the test. These falsifications were allegedly performed for anywhere from $10 to $100 over and above the usual emissions testing fee.    The U.S. Environmental Protection Agency (EPA), under the Clean Air Act, requires the state of Nevada to conduct vehicle emissions testing in certain areas because the areas exceed national standards for carbon monoxide and ozone. Las Vegas is currently required to perform emissions testing.    To obtain a registration renewal, vehicle owners bring the vehicles to a licensed inspection station for testing. The emissions inspector logs into a computer to activate the system by using a unique password issued to the emissions inspector. The emissions inspector manually inputs the vehicle’s VIN to identify the tested vehicle, then connects the vehicle for model year 1996 and later to an onboard diagnostics port connected to an analyzer. The analyzer downloads data from the vehicle’s computer, analyzes the data and provides a \"pass\" or \"fail\" result. The pass or fail result and vehicle identification data are reported on the Vehicle Inspection Report. It is a crime to knowingly alter or conceal any record or other document required to be maintained by the Clean Air Act.     \"Falsifications of vehicle emissions testing, such as those alleged in the indictments unsealed today, are serious matters and we intend to use all of our enforcement tools to stop this harmful practice. These actions undermine a system that is designed to reduce air pollutants including smog and provide better air quality for the citizens of Nevada,\" said Ignacia S. Moreno, Assistant Attorney General for the Justice Department’s Environment and Natural Resources Division.    \"The residents of Nevada deserve to know that the vast majority of licensed vehicle emission inspectors are not corrupt and are not circumventing emission testing procedures,\" said U.S. Attorney Bogden. \"These indictments should serve as a clear warning to offenders that the Department of Justice will prosecute you if you make fraudulent statements and reports concerning compliance with the federal Clean Air Act.\"    \"Lying about car emissions means dirtier air, which is especially of concern in areas like Las Vegas that are already experiencing air quality problems,\" said Cynthia Giles, Assistant Administrator for Enforcement and Compliance Assurance at EPA. \"We will take aggressive action to ensure communities have clean air.\"    The maximum penalty for the felony violations contained in the indictments includes up to two years in prison and a fine of up to $250,000.    An indictment is merely an accusation, and a defendant is presumed innocent unless and until proven guilty in a court of law.    The case was investigated by the EPA, Criminal Investigation Division; and the Nevada Department of Motor Vehicles Compliance Enforcement Division. The case is being prosecuted by the U.S. Attorney’s Office for the District of Nevada and the Justice Department’s Environmental Crimes Section.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The U.S. Department of Justice, the U.S. Environmental Protection Agency (EPA), and the Rhode Island Department of Environmental Management (RIDEM) announced today that two subsidiaries of Stanley Black & Decker Inc.—Emhart Industries Inc. and Black & Decker Inc.—have agreed to clean up dioxin contaminated sediment and soil at the Centredale Manor Restoration Project Superfund Site in North Providence and Johnston, Rhode Island.  “We are pleased to reach a resolution through collaborative work with the responsible parties, EPA, and other stakeholders,” said Acting Assistant Attorney General Jeffrey H. Wood for the Justice Department's Environment and Natural Resources Division . “Today’s settlement ends protracted litigation and allows for important work to get underway to restore a healthy environment for citizens living in and around the Centredale Manor Site and the Woonasquatucket River.” “This settlement demonstrates the tremendous progress we are achieving working with responsible parties, states, and our federal partners to expedite sites through the entire Superfund remediation process,” said EPA Acting Administrator Andrew Wheeler. “The Centredale Manor Site has been on the National Priorities List for 18 years; we are taking charge and ensuring the Agency makes good on its promise to clean it up for the betterment of the environment and those communities affected.” “Successfully concluding this settlement paves the way for EPA to make good on our commitment to aggressively pursue cleaning up the Centredale Manor Superfund Site,” said EPA New England Regional Administrator Alexandra Dunn. “We are excited to get to work on the cleanup at this site, and get it closer to the goal of being fully utilized by the North Providence and Johnston communities.” “We are pleased that the collective efforts of the State of Rhode Island, EPA, and DOJ in these negotiations have concluded in this major milestone toward the cleanup of the Centredale Manor Restoration Superfund site and are consistent with our long-standing efforts to make the polluter pay,” said RIDEM Director Janet Coit. “The settlement will speed up a remedy that protects public health and the river environment, and moves us closer to the day that we can reclaim recreational uses of this beautiful river resource.” The settlement, which includes cleanup work in the Woonasquatucket River (River) and bordering residential and commercial properties along the River, requires the companies to perform the remedy selected by EPA for the Site in 2012, which is estimated to cost approximately $100 million, and resolves longstanding litigation. The cleanup remedy includes excavation of contaminated sediment and floodplain soil from the Woonasquatucket River, including from adjacent residential properties. Once the cleanup remedy is completed, full access to the Woonasquatucket River should be restored for local citizens. The cleanup will be a step toward the State’s goal of a fishable and swimmable river. The work will also include upgrading caps over contaminated soil in the peninsula area of the Site that currently house two high-rise apartment buildings. The settlement also ensures that the long-term monitoring and maintenance of the site, as directed in the remedy, will be implemented to ensure that public health is protected.  Under the settlement, Emhart and Black & Decker will reimburse EPA for approximately $42 million in past costs incurred at the Site. The companies will also reimburse EPA and the State of Rhode Island for future costs incurred by those agencies in overseeing the work required by the settlement. The settlement will also include payments on behalf of two federal agencies to resolve claims against those agencies. These payments, along with prior settlements related to the Site, will result in a 100 percent recovery for the United States of its past and future response costs related to the Site. Litigation related to the Site has been ongoing for nearly eight years. While the Federal District Court found Black & Decker and Emhart to be liable for their hazardous waste and responsible to conduct the cleanup of the Site, it had also ruled that EPA needed to reconsider certain aspects of that cleanup. EPA appealed the decision requiring it to reconsider aspects of the cleanup. This settlement, once entered by the District Court, will resolve the litigation between the United States, Rhode Island, and Emhart and Black and Decker, allowing the cleanup of the Site to begin. The Site spans a one and a half mile stretch of the Woonasquatucket River and encompasses a nine-acre peninsula, two ponds and a significant forested wetland. From the 1940s to the early 1970s, Emhart’s predecessor operated a chemical manufacturing facility on the peninsula and used a raw material that was contaminated with 2,3,7,8-tetrachlorodibenzo-p-dioxin, a toxic form of dioxin. The Site property was also previously used by a barrel refurbisher. Elevated levels of dioxins and other contaminants have been detected in soil, groundwater, sediment, surface water and fish.  The Site was added to the National Priorities List (NPL) in 2000, and in December 2017, EPA included the Centredale Manor Restoration Project Superfund Site on a list of Superfund sites targeted for immediate and intense attention. Several short-term actions were previously performed at the Site to address immediate threats to the residents and minimize potential erosion and downstream transport of contaminated soil and sediment. This settlement is the latest agreement EPA has reached since the Site was listed on the NPL. Prior agreements addressed the performance and recovery of costs for the past environmental investigations and interim cleanup actions from Emhart, the barrel reconditioning company, the current owners of the peninsula portion of the Site, and other potentially responsible parties. The Consent Decree, lodged in the U.S. District Court of Rhode Island, will be posted in the Federal Register and available for public comment for a period of 30 days. The Consent Decree can be viewed on the Justice Department website: www.justice.gov/enrd/Consent_Decrees.html.  EPA information on the Centredale Manor Superfund Site: www.epa.gov/superfund/centredale.   \n",
       "\n",
       "                        date topics_clean  \\\n",
       "0  2014-10-01T00:00:00-04:00     No topic   \n",
       "1  2012-07-25T00:00:00-04:00     No topic   \n",
       "2  2011-08-03T00:00:00-04:00     No topic   \n",
       "3  2010-01-08T00:00:00-05:00     No topic   \n",
       "4  2018-07-09T00:00:00-04:00  Environment   \n",
       "\n",
       "                             components_clean  \n",
       "0            National Security Division (NSD)  \n",
       "1  Environment and Natural Resources Division  \n",
       "2  Environment and Natural Resources Division  \n",
       "3  Environment and Natural Resources Division  \n",
       "4  Environment and Natural Resources Division  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first, unzip the file pset3_inputdata.zip \n",
    "## then, run this code to load the unzipped json file and convert to a dataframe\n",
    "## (may need to change the pathname depending on where you store stuff)\n",
    "## and convert some of the attributes from lists to values\n",
    "doj = pd.read_json(\"combined.json\", lines = True)\n",
    "\n",
    "## due to json, topics are in a list so remove them and concatenate with ;\n",
    "doj['topics_clean'] = [\"; \".join(topic) \n",
    "                      if len(topic) > 0 else \"No topic\" \n",
    "                      for topic in doj.topics]\n",
    "\n",
    "## similarly with components\n",
    "doj['components_clean'] = [\"; \".join(comp) \n",
    "                           if len(comp) > 0 else \"No component\" \n",
    "                           for comp in doj.components]\n",
    "\n",
    "## drop older columns from data\n",
    "doj = doj[['id', 'title', 'contents', 'date', 'topics_clean', \n",
    "           'components_clean']].copy()\n",
    "\n",
    "doj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tagging and sentiment scoring (17 points)\n",
    "\n",
    "Focus on the following press release: `id` == \"17-1204\" about this pharmaceutical kickback prosecution: https://www.forbes.com/sites/michelatindera/2017/11/16/fentanyl-billionaire-john-kapoor-to-plead-not-guilty-in-opioid-kickback-case/?sh=21b8574d6c6c \n",
    "\n",
    "The `contents` column is the one we're treating as a document. You may need to to convert it from a pandas series to a single string.\n",
    "\n",
    "We'll call the raw string of this press release `pharma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code to subset to one press release and take the string\n",
    "\n",
    "subset = doj[doj['id'] == '17-1204']['contents']\n",
    "\n",
    "pharma = subset.str.cat(sep=' ')\n",
    "\n",
    "## proving conversion from panda series to a string \n",
    "type(pharma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 part of speech tagging (3 points)\n",
    "\n",
    "A. Preprocess the `pharma` press release to remove all punctuation / digits (you can use `.isalpha()` to subset)\n",
    "\n",
    "B. With the preprocessed press release from part A, use the part of speech tagger within nltk to tag all the words in that one press release with their part of speech. \n",
    "\n",
    "C. Using the output from B, extract the adjectives and sort those adjectives from most occurrences to fewest occurrences. Print a dataframe with the 5 most frequent adjectives and their counts in the `pharma` release. See here for a list of the names of adjectives within nltk: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "**Resources**:\n",
    "\n",
    "- Documentation for `.isalpha()`: https://www.w3schools.com/python/ref_string_isalpha.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The founder and majority owner of Insys Therapeutics Inc was arrested today and charged with leading a nationwide conspiracy to profit by using bribes and fraud to cause the illegal distribution of a Fentanyl spray intended for cancer patients experiencing breakthrough pain More than  Americans died of synthetic opioid overdoses last year and millions are addicted to opioids And yet some medical professionals would rather take advantage of the addicts than try to help them said Attorney General Jeff Sessions This Justice Department will not tolerate this We will hold accountable anyone  from street dealers to corporate executives  who illegally contributes to this nationwide epidemic And under the leadership of President Trump we are fully committed to defeating this threat to the American peopleJohn N Kapoor  of Phoenix Ariz a current member of the Board of Directors of Insys was arrested this morning in Arizona and charged with RICO conspiracy as well as other felonies including conspiracy to commit mail and wire fraud and conspiracy to violate the AntiKickback Law Kapoor the former Executive Chairman of the Board and CEO of Insys will appear in federal court in Phoenix today He will appear in US District Court in Boston at a later date The superseding indictment unsealed today in Boston also includes additional allegations against several former Insys executives and managers who were initially indicted in December The superseding indictment charges that Kapoor Michael L Babich  of Scottsdale Ariz former CEO and President of the company Alec Burlakoff  of Charlotte NC former Vice President of Sales Richard M Simon  of Seal Beach Calif former National Director of Sales former Regional Sales Directors Sunrise Lee  of Bryant City Mich and Joseph A Rowan  of Panama City Fla and former Vice President of Managed Markets Michael J Gurry  of Scottsdale Ariz conspired to bribe practitioners in various states many of whom operated pain clinics in order to get them to prescribe a fentanylbased pain medication The medication called Subsys is a powerful narcotic intended to treat cancer patients suffering intense breakthrough pain In exchange for bribes and kickbacks the practitioners wrote large numbers of prescriptions for the patients most of whom were not diagnosed with cancerThe indictment also alleges that Kapoor and the six former executives conspired to mislead and defraud health insurance providers who were reluctant to approve payment for the drug when it was prescribed for noncancer patients They achieved this goal by setting up the reimbursement unit which was dedicated to obtaining prior authorization directly from insurers and pharmacy benefit managers In the midst of a nationwide opioid epidemic that has reached crisis proportions Mr Kapoor and his company stand accused of bribing doctors to overprescribe a potent opioid and committing fraud on insurance companies solely for profit said Acting United States Attorney William D Weinreb Todays arrest and charges reflect our ongoing efforts to attack the opioid crisis from all angles We must hold the industry and its leadership accountable  just as we would the cartels or a streetlevel drug dealerAs alleged these executives created a corporate culture at Insys that utilized deception and bribery as an acceptable business practice deceiving patients and conspiring with doctors and insurers said Harold H Shaw Special Agent in Charge of the Federal Bureau of Investigation Boston Field Division The allegations of selling a highly addictive opioid cancer pain drug to patients who did not have cancer make them no better than streetlevel drug dealers Todays charges mark an important step in holding pharmaceutical executives responsible for their part in the opioid crisis The FBI will vigorously investigate corrupt organizations with business practices that promote fraud with a total disregard for patient safetyThese Insys executives allegedly fueled the opioid epidemic by paying doctors to needlessly prescribe an extremely dangerous and addictive form of fentanyl said Phillip Coyne Special Agent in Charge for the Office of Inspector General of the US Department of Health and Human Services Corporate executives intent on illegally driving up profits need to be aware they are now squarely in the sights of law enforcementAs alleged Insys executives improperly influenced health care providers to prescribe a powerful opioid for patients who did not need it and without complying with FDA requirements thus putting patients at risk and contributing to the current opioid crisis said Mark A McCormack Special Agent in Charge FDA Office of Criminal Investigations Metro Washington Field Office Our office will continue to work with our law enforcement partners to pursue and bring to justice those who threaten the public healthPharmaceutical companies whose products include controlled medications that can lead to addiction and overdose have a special obligation to operate in a trustworthy transparent manner because their customers health and safety and indeed very lives depend on it said DEA Special Agent in Charge Michael J Ferguson DEA pledges to work with our law enforcement and regulatory partners nationwide to ensure that rules and regulations under the Controlled Substances Act are followedTodays arrest is the result of a joint effort to identify investigate and prosecute individuals who engage in fraudulent activity and endanger patient health stated Special Agent in Charge LeighAlistair Barzey Defense Criminal Investigative Service DCIS Northeast Field Office DCIS will continue to work with the US Attorneys Office District of Massachusetts and our law enforcement partners to protect US military members retirees and their dependents and the integrity of TRICARE the Defense Departments healthcare systemAs alleged John Kapoor and other top executives committed fraud placing profit before patient safety to sell a highly potent and addictive opioid EBSA will take every opportunity to work collaboratively with our law enforcement partners in these important investigations to protect participants in private sector health plans and contribute in fighting the opioid epidemic said Susan A Hensley Regional Director of the US Department of Labor Employee Benefits Security Administration Boston Regional OfficeOnce again the United States Postal Inspection Service is fully committed to protecting our nations mail system from criminal misuse said Shelly Binkowski Inspector in Charge of the US Postal Inspection Service We are proud to work alongside our law enforcement partners to dismantle high level prescription drug practices which directly contribute to the opioid abuse epidemic This investigation highlights our commitment to defending our mail system from illegal misuse and ensuring public trust in the mailThe US Department of Veterans Affairs Office of Inspector General will continue to aggressively investigate those that attempt to fraudulently impact programs designed to benefit our veterans and their families said Donna L Neves Special Agent in Charge of the VA OIG Northeast Field OfficeThe charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than  years in prison three years of supervised release and a fine of  or twice the amount of pecuniary gain or loss The charges of conspiracy to violate the AntiKickback Law provide for a sentence of no greater than five years in prison three years of supervised release and a  fine Sentences are imposed by a federal district court judge based upon the US Sentencing Guidelines and other statutory factorsThe investigation was conducted by a team that included the FBI HHSOIG FDA Office of Criminal Investigations the Defense Criminal Investigative Service the Drug Enforcement Administration the Department of Labor Employee Benefits Security Administration the Office of Personnel Management the US Postal Inspection Service the US Postal Service Office of Inspector General and the Department of Veterans Affairs The US Attorneys Office would like to acknowledge the cooperation and assistance of the US Attorneys Offices around the country engaged in parallel investigations including the District of Connecticut Eastern District of Michigan Southern District of Alabama Southern District of New York District of Rhode Island and the District of New Hampshire The efforts of the Central District of California and the Justice Departments Civil Fraud Section of the Department of Justice are also greatly appreciated Assistant US Attorneys K Nathaniel Yeager Chief of Weinrebs Health Care Fraud Unit and Susan M Poswistilo of Weinrebs Civil Division are prosecuting the caseThe details contained in the charging documents are allegations The defendants are presumed innocent unless and until proven guilty beyond a reasonable doubt'"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A: your code here to restrict to alpha\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    punctuation = set(string.punctuation)\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    cleaned_words = [''.join(char for char in word if char.isalpha()) for word in words]\n",
    "    \n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "preprocess_text(pharma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B: your code here for part of speech tagging\n",
    "\n",
    "tokens = word_tokenize(pharma) \n",
    "\n",
    "tokens_pos = pos_tag(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nationwide': 4, 'illegal': 2, 'More': 1, 'synthetic': 1, 'last': 1, 'medical': 1, 'accountable': 2, 'corporate': 2, 'American': 1, 'current': 2, 'other': 3, 'former': 8, 'federal': 2, 'later': 1, 'unsealed': 1, 'additional': 1, 'several': 1, 'various': 1, 'many': 1, 'fentanyl-based': 1, 'powerful': 2, 'narcotic': 1, 'intense': 1, 'large': 1, 'most': 1, 'reluctant': 1, 'non-cancer': 1, '“': 1, 'prior': 1, 'potent': 2, 'ongoing': 1, 'opioid': 5, 'street-level': 2, 'utilized': 1, 'acceptable': 1, 'addictive': 3, 'better': 1, 'important': 2, 'pharmaceutical': 1, 'responsible': 1, 'corrupt': 1, 'total': 1, 'patient': 2, 'epidemic': 1, 'dangerous': 1, 'Corporate': 1, 'aware': 1, 'public': 2, 'controlled': 1, 'special': 1, 'transparent': 1, 'regulatory': 1, 'followed.': 1, 'joint': 1, 'prosecute': 1, 'fraudulent': 1, 'Special': 1, 'military': 1, 'system.': 1, 'top': 1, 'private': 1, 'criminal': 1, 'proud': 1, 'high': 1, 'impact': 1, 'greater': 2, 'supervised': 2, 'pecuniary': 1, 'statutory': 1, 'factors.The': 1, 'parallel': 1, 'innocent': 1, 'guilty': 1, 'reasonable': 1}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[371], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         adj_counts[adj] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(adj_counts)\n\u001b[0;32m---> 14\u001b[0m sorted_adj_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(adjectives\u001b[38;5;241m.\u001b[39mitems(), key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m item: item[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# sort from most to fewest occurrences \u001b[39;00m\n\u001b[1;32m     17\u001b[0m most_common_adjectives \u001b[38;5;241m=\u001b[39m sorted_adj_counts[:\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "## C: your code here to extract adjectives \n",
    "\n",
    "adjectives = [word for word, pos in tokens_pos if pos.startswith('JJ')]\n",
    "\n",
    "adj_counts = {}\n",
    "\n",
    "for adj in adjectives:\n",
    "    if adj in adj_counts:\n",
    "        adj_counts[adj] += 1\n",
    "    else:\n",
    "        adj_counts[adj] = 1\n",
    "print(adj_counts)\n",
    "\n",
    "sorted_adj_counts = sorted(adjectives.items(), key = lambda item: item[1], reverse=True)\n",
    "\n",
    "# sort from most to fewest occurrences \n",
    "most_common_adjectives = sorted_adj_counts[:5]\n",
    "most_common_adjectives\n",
    "\n",
    "df_adjectives = pd.DataFrame(sorted_adj_counts, columns=['adjective', 'count'])\n",
    "print(df_adjectives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 named entity recognition (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the original `pharma` press release (so the one before stripping punctuation/digits), use spaCy to extract all named entities from the press release.\n",
    "\n",
    "B. Print the unique named entities with the tag: `LAW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Insys Therapeutics Inc.; NER tag: ORG\n",
      "Entity: today; NER tag: DATE\n",
      "Entity: Fentanyl; NER tag: PERSON\n",
      "Entity: More than 20,000; NER tag: CARDINAL\n",
      "Entity: Americans; NER tag: NORP\n",
      "Entity: last year; NER tag: DATE\n",
      "Entity: millions; NER tag: CARDINAL\n",
      "Entity: Jeff Sessions; NER tag: PERSON\n",
      "Entity: This Justice Department; NER tag: ORG\n",
      "Entity: Trump; NER tag: PERSON\n",
      "Entity: American; NER tag: NORP\n",
      "Entity: ”John N. Kapoor; NER tag: PERSON\n",
      "Entity: 74; NER tag: DATE\n",
      "Entity: Phoenix; NER tag: GPE\n",
      "Entity: Ariz.; NER tag: GPE\n",
      "Entity: the Board of Directors; NER tag: ORG\n",
      "Entity: Insys; NER tag: ORG\n",
      "Entity: this morning; NER tag: TIME\n",
      "Entity: Arizona; NER tag: GPE\n",
      "Entity: RICO; NER tag: LAW\n",
      "Entity: Kapoor; NER tag: PERSON\n",
      "Entity: Executive; NER tag: ORG\n",
      "Entity: Board; NER tag: ORG\n",
      "Entity: Insys; NER tag: ORG\n",
      "Entity: Phoenix; NER tag: GPE\n",
      "Entity: today; NER tag: DATE\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: District Court; NER tag: ORG\n",
      "Entity: Boston; NER tag: GPE\n",
      "Entity: a later date; NER tag: DATE\n",
      "Entity: today; NER tag: DATE\n",
      "Entity: Boston; NER tag: GPE\n",
      "Entity: Insys; NER tag: ORG\n",
      "Entity: December 2016.The; NER tag: DATE\n",
      "Entity: Kapoor; NER tag: GPE\n",
      "Entity: Michael L. Babich; NER tag: PERSON\n",
      "Entity: 40; NER tag: DATE\n",
      "Entity: Scottsdale; NER tag: GPE\n",
      "Entity: Ariz.; NER tag: GPE\n",
      "Entity: Alec Burlakoff; NER tag: PERSON\n",
      "Entity: 42; NER tag: DATE\n",
      "Entity: Charlotte; NER tag: GPE\n",
      "Entity: N.C.; NER tag: GPE\n",
      "Entity: Richard M. Simon; NER tag: PERSON\n",
      "Entity: 46; NER tag: DATE\n",
      "Entity: Seal Beach; NER tag: GPE\n",
      "Entity: Calif.; NER tag: GPE\n",
      "Entity: Sunrise Lee; NER tag: PERSON\n",
      "Entity: 36; NER tag: DATE\n",
      "Entity: Bryant City; NER tag: GPE\n",
      "Entity: Mich.; NER tag: GPE\n",
      "Entity: Joseph A. Rowan; NER tag: PERSON\n",
      "Entity: 43; NER tag: DATE\n",
      "Entity: Panama City; NER tag: GPE\n",
      "Entity: Fla.; NER tag: GPE\n",
      "Entity: Managed Markets; NER tag: ORG\n",
      "Entity: Michael J. Gurry; NER tag: PERSON\n",
      "Entity: 53; NER tag: DATE\n",
      "Entity: Scottsdale; NER tag: GPE\n",
      "Entity: Ariz.; NER tag: GPE\n",
      "Entity: Subsys; NER tag: ORG\n",
      "Entity: Kapoor; NER tag: GPE\n",
      "Entity: six; NER tag: CARDINAL\n",
      "Entity: Kapoor; NER tag: PERSON\n",
      "Entity: United States; NER tag: GPE\n",
      "Entity: William D. Weinreb; NER tag: PERSON\n",
      "Entity: Today; NER tag: DATE\n",
      "Entity: Insys; NER tag: ORG\n",
      "Entity: Harold H. Shaw; NER tag: PERSON\n",
      "Entity: Charge of the Federal Bureau of Investigation; NER tag: ORG\n",
      "Entity: Boston Field Division; NER tag: ORG\n",
      "Entity: Today; NER tag: DATE\n",
      "Entity: FBI; NER tag: ORG\n",
      "Entity: Insys; NER tag: PERSON\n",
      "Entity: Phillip Coyne; NER tag: PERSON\n",
      "Entity: Charge for the Office of Inspector General of the U.S. Department of Health and Human Services; NER tag: ORG\n",
      "Entity: Insys; NER tag: ORG\n",
      "Entity: FDA; NER tag: ORG\n",
      "Entity: Mark A. McCormack; NER tag: PERSON\n",
      "Entity: Charge; NER tag: GPE\n",
      "Entity: FDA Office of Criminal Investigations’ Metro; NER tag: ORG\n",
      "Entity: Field Office; NER tag: ORG\n",
      "Entity: DEA Special; NER tag: ORG\n",
      "Entity: Michael J. Ferguson; NER tag: PERSON\n",
      "Entity: DEA; NER tag: ORG\n",
      "Entity: the Controlled Substances Act; NER tag: LAW\n",
      "Entity: Charge Leigh-Alistair Barzey; NER tag: PERSON\n",
      "Entity: Defense Criminal Investigative Service; NER tag: ORG\n",
      "Entity: Northeast Field Office; NER tag: ORG\n",
      "Entity: DCIS; NER tag: ORG\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: Massachusetts; NER tag: GPE\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: TRICARE; NER tag: ORG\n",
      "Entity: the Defense Department’s; NER tag: ORG\n",
      "Entity: John Kapoor; NER tag: PERSON\n",
      "Entity: Susan A. Hensley; NER tag: PERSON\n",
      "Entity: the U.S. Department of Labor; NER tag: ORG\n",
      "Entity: Employee Benefits Security Administration; NER tag: ORG\n",
      "Entity: Boston Regional Office; NER tag: ORG\n",
      "Entity: the United States Postal Inspection Service; NER tag: ORG\n",
      "Entity: Shelly Binkowski; NER tag: PERSON\n",
      "Entity: the U.S. Postal Inspection Service; NER tag: ORG\n",
      "Entity: U.S. Department; NER tag: ORG\n",
      "Entity: Veterans Affairs; NER tag: ORG\n",
      "Entity: Office of Inspector General; NER tag: ORG\n",
      "Entity: Donna L. Neves; NER tag: PERSON\n",
      "Entity: Charge of the VA OIG Northeast Field Office; NER tag: ORG\n",
      "Entity: RICO; NER tag: LAW\n",
      "Entity: 20 years; NER tag: DATE\n",
      "Entity: three years; NER tag: DATE\n",
      "Entity: 250,000; NER tag: MONEY\n",
      "Entity: five years; NER tag: DATE\n",
      "Entity: three years; NER tag: DATE\n",
      "Entity: 25,000; NER tag: MONEY\n",
      "Entity: the U.S. Sentencing Guidelines; NER tag: ORG\n",
      "Entity: FBI; NER tag: ORG\n",
      "Entity: HHS-OIG; NER tag: ORG\n",
      "Entity: FDA Office of Criminal Investigations; NER tag: ORG\n",
      "Entity: the Defense Criminal Investigative Service; NER tag: ORG\n",
      "Entity: the Drug Enforcement Administration; NER tag: ORG\n",
      "Entity: the Department of Labor; NER tag: ORG\n",
      "Entity: Employee Benefits Security Administration; NER tag: ORG\n",
      "Entity: the Office of Personnel Management; NER tag: ORG\n",
      "Entity: the U.S. Postal Inspection Service; NER tag: ORG\n",
      "Entity: the U.S. Postal Service Office of Inspector General; NER tag: ORG\n",
      "Entity: the Department of Veterans Affairs; NER tag: ORG\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: the District of Connecticut; NER tag: GPE\n",
      "Entity: Eastern District; NER tag: LOC\n",
      "Entity: Michigan; NER tag: GPE\n",
      "Entity: Southern District; NER tag: LOC\n",
      "Entity: Southern District; NER tag: LOC\n",
      "Entity: New York; NER tag: GPE\n",
      "Entity: District of; NER tag: GPE\n",
      "Entity: Rhode Island; NER tag: GPE\n",
      "Entity: the District of New Hampshire; NER tag: GPE\n",
      "Entity: the Central District; NER tag: LOC\n",
      "Entity: California; NER tag: GPE\n",
      "Entity: the Justice Department’s; NER tag: ORG\n",
      "Entity: Civil Fraud Section; NER tag: ORG\n",
      "Entity: the Department of Justice; NER tag: ORG\n",
      "Entity: U.S.; NER tag: GPE\n",
      "Entity: Nathaniel Yeager; NER tag: PERSON\n",
      "Entity: Weinreb’s Health Care Fraud Unit; NER tag: ORG\n",
      "Entity: Susan M. Poswistilo; NER tag: PERSON\n",
      "Entity: Weinreb’s Civil Division; NER tag: ORG\n"
     ]
    }
   ],
   "source": [
    "## your code here for part A\n",
    "\n",
    "## resetting pharma \n",
    "\n",
    "subset_new = doj[doj['id'] == '17-1204']['contents']\n",
    "\n",
    "pharma_new = subset.str.cat(sep=' ')\n",
    "\n",
    "## using spacy \n",
    "\n",
    "spacy_pharma = nlp(pharma_new)\n",
    "\n",
    "for one_tok in spacy_pharma.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the Controlled Substances Act', 'RICO'}\n"
     ]
    }
   ],
   "source": [
    "## your code here for part B\n",
    "\n",
    "law_ents = set(ent.text for ent in spacy_pharma.ents if ent.label_ == 'LAW')\n",
    "\n",
    "print(law_ents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use Google to summarize in one sentence what the `RICO` named entity means and why this might apply to a pharmaceutical kickbacks case (and not just a mafia case...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C: summarizing RICO\n",
    "\n",
    "# RICO, or the Racketeer Influenced and Corrupt Organizations, was a law passed in the 1970s primarily intended to target mafia cases, but \n",
    "# the flexibility of its statuatory language (engagement in a pattern of criminal activity connected to an enterprise) has allowed it to be \n",
    "# applied to cases involving drug cartels, corportations, and even politicians\n",
    "\n",
    "# Link to Source Website: https://www.justia.com/criminal/docs/rico/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. You want to extract the possible sentence lengths the CEO is facing; pull out the named entities with (1) the label `DATE` and (2) that contain the word year or years (hint: you may want to use the `re` module for that second part). Print these named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last year', '20 years', 'three years', 'five years', 'three years']\n"
     ]
    }
   ],
   "source": [
    "## D: extracting possible sentence lengths \n",
    "\n",
    "date_ents = [ent.text for ent in spacy_pharma.ents if ent.label_ == 'DATE' and re.search(r'\\b(?:year|years)\\b', ent.text, re.IGNORECASE)]\n",
    "\n",
    "print(date_ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Pull and print the original parts of the press releases where those year lengths are mentioned (e.g., the sentences or rough region of the press release). Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) the CEO may be facing if convicted after this indictment (if there are multiple lengths mentioned describe the maximum). \n",
    "\n",
    "**Hint**: you may want to use re.search or re.findall \n",
    "\n",
    "- For part E, you can use `re.search` and `re.findall`, or anything that works 😳."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than 20 years in prison, three years of supervised release and a fine of $250,000, or twice the amount of pecuniary gain or loss.',\n",
       " '\\xa0 The charges of conspiracy to violate the Anti-Kickback Law provide for a sentence of no greater than five years in prison, three years of supervised release and a $25,000 fine.']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The charges of conspiracy to commit RICO and conspiracy to commit mail and wire fraud each provide for a sentence of no greater than 20 years in prison, three years of supervised release and a fine of $250,000, or twice the amount of pecuniary gain or loss.\n",
      "  The charges of conspiracy to violate the Anti-Kickback Law provide for a sentence of no greater than five years in prison, three years of supervised release and a $25,000 fine.\n"
     ]
    }
   ],
   "source": [
    "## E: pull and print the original parts of the press releases where those year lengths are mentioned\n",
    "\n",
    "pattern = r'([^.]*?years[^.]*\\.)'\n",
    "\n",
    "# adjust regex pattern so I'm matching to sentence! \n",
    "\n",
    "all_matchez = re.findall(pattern, pharma_new)\n",
    "\n",
    "all_matchez\n",
    "\n",
    "for match in all_matchez:\n",
    "    print(match)\n",
    "\n",
    "\n",
    "## Question: Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) the CEO may be \n",
    "## facing if convicted after this indictment (if there are multiple lengths mentioned describe the maximum).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 sentiment analysis  (10 points)\n",
    "\n",
    "A. Subset the press releases to those labeled with one of three topics via `topics_clean`: Civil Rights, Hate Crimes, and Project Safe Childhood. We'll call this `doj_subset` going forward and it should have 717 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(717, 7)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## your code here for subsetting\n",
    "\n",
    "doj_subset = doj[doj[\"topics_clean\"].isin([\"Civil Rights\", \"Hate Crimes\", \"Project Safe Childhood\"])].reset_index()\n",
    "\n",
    "doj_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Write a function that takes one press release string as an input and:\n",
    "\n",
    "- Removes named entities from each press release string (**Hint**: you may want to use `re.sub` with an or condition)\n",
    "- Scores the sentiment of the entire press release using the `SentimentIntensityAnalyzer` and `polarity_scores`\n",
    "- Returns the length-four (negative, positive, neutral, compound) sentiment dictionary (any order is fine)\n",
    "\n",
    "Apply that function to each of the press releases in `doj_subset`. \n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- A function + list comprehension to execute will takes about 30 seconds on a respectable local machine and about 2 mins on jhub; if it's taking a very long time, you may want to check your code for inefficiencies. If you can't fix those, for partial credit on this part/full credit on remainder, you can take a small random sample of the 717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_and_analyze_sentiment(text):\n",
    "\n",
    "    tokenized = word_tokenize(text)\n",
    "    tagged = pos_tag(tokenized)\n",
    "    \n",
    "    nlp_res = nlp(text)\n",
    "    without_ner = \" \".join([ent.text for ent in nlp_res if not ent.ent_type_])\n",
    "\n",
    "    sent = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # sent score\n",
    "    sentiment_scores = sent.polarity_scores(without_ner)\n",
    "    \n",
    "    # dictionary\n",
    "    return sentiment_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A former supervisory correctional officer at Louisiana State Penitentiary in Angola, Louisiana, pleaded guilty yesterday in connection with the beating of a handcuffed and shackled inmate, in addition to conspiring to cover up their misconduct by falsifying official records and lying to internal investigators about what happened.\\xa0  \\xa0 James Savoy, 39, of Marksville, Louisiana, admitted during his plea hearing that he witnessed other officers using excessive force against the inmate and failed to intervene; that he conspired with other officers to cover up the beating by engaging in a variety of obstructive acts; and that he personally falsified official prison records to cover up the attack. \\xa0 Scotty Kennedy, 48, of Beebe, Arkansas, and John Sanders, 30, of Marksville, Louisiana previously pleaded guilty in November 2016, and September 2017, for their roles in the beating and cover up. \\xa0 “Every citizen has the right to due process and protection from unreasonable force, and correctional officers who violate these basic Constitutional rights must be held accountable for their egregious actions” said Acting Assistant Attorney General John Gore of the Civil Rights Division.\\xa0 “The Justice Department will continue to vigorously prosecute correctional officers who violate the public’s trust by committing crimes and to covering up violations of federal criminal law.” \\xa0 “Yesterday is another example of our office’s unwavering commitment to pursuing those who violate the federal criminal civil rights laws,” said Acting United States Attorney for the Middle District of Louisiana Corey Amundson. “We will continue to work closely with the Justice Department’s Civil Rights Division and the FBI to ensure that no one is above the law.”\\xa0  \\xa0 This case is being investigated by the FBI’s Baton Rouge Resident Agency and is being prosecuted by Assistant U.S. Attorney Frederick A. Menner, Jr. of the Middle District of Louisiana and Trial Attorney Christopher J. Perras of the Civil Rights Division’s Criminal Section.'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992},\n",
       " {'neg': 0.18, 'neu': 0.773, 'pos': 0.047, 'compound': -0.992}]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## B: your code here \n",
    "\n",
    "subset_3 = doj_subset[doj_subset['id'] == '17-1235']['contents']\n",
    "\n",
    "pharma_3 = subset_3.str.cat(sep=' ')\n",
    "\n",
    "pharma_3\n",
    "\n",
    "## testing this on a specific case\n",
    "\n",
    "results = [clean_and_analyze_sentiment(pharma_3) for contents in doj_subset]\n",
    "\n",
    "results\n",
    "\n",
    "## applying this to doj_subset \n",
    "\n",
    "doj_subset[\"sentiment_scores\"] = doj_subset[\"contents\"].apply(clean_and_analyze_sentiment)\n",
    "print(doj_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Add the four sentiment scores to the `doj_subset` dataframe to create a dataframe: `doj_subset_wscore`. Sort from highest neg to lowest neg score and print the top `id`, `contents`, and `neg` columns of the two most neg press releases. \n",
    "\n",
    "Notes:\n",
    "\n",
    "- Don't worry if your sentiment score differs slightly from our output on GitHub; differences in preprocessing can lead to diff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C: working with the four sentiment scores \n",
    "\n",
    "doj_subset_wscore = doj_subset.copy().reset_index()\n",
    "\n",
    "sentiment_scores = doj_subset_wscore['contents'].apply(clean_and_analyze_sentiment)\n",
    "doj_subset_wscore['neg_score'] = doj_subset_wscore[\"sentiment_scores\"].apply(lambda x: x['neg'])\n",
    "doj_subset_wscore['pos_score'] = doj_subset_wscore[\"sentiment_scores\"].apply(lambda x: x['pos'])\n",
    "doj_subset_wscore['neu_score'] = doj_subset_wscore[\"sentiment_scores\"].apply(lambda x: x['neu'])\n",
    "doj_subset_wscore['compound_score'] = sdoj_subset_wscore[\"sentiment_scores\"].apply(lambda x: x['compound'])\n",
    "\n",
    "## sorting from highest to lowest\n",
    "\n",
    "doj_subset_wscore = doj_subset_wscore.sort_values(by = 'neg', ascending = False)\n",
    "\n",
    "# print of most negative press releases\n",
    "top_neg = doj_subset_wscore.head(2)\n",
    "\n",
    "top_neg[['id', 'contents', 'neg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. With the dataframe from part C, find the mean compound sentiment score for each of the three topics in `topics_clean` using group_by and agg.\n",
    "\n",
    "E. Add a 1 sentence interpretation of why we might see the variation in scores (remember that compound is a standardized summary where -1 is most negative; +1 is most positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## D: agg and find the mean compound score by topic\n",
    "\n",
    "print(\"Mean Compound Sentiment Score by Topic\", doj_subset_wscore.groupby(\"topics_clean\")['compound_score'].agg('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E: interpreting results \n",
    "\n",
    "# We see variation in scores because of the connotations of the three categories. There is an overwhelmingly positive connotation associated with\n",
    "# civil rights, so it makes sense the the sentiment score is positive. There is a strong negative connotation associated with hate crimes, thus\n",
    "# it makes sense that the sentiment scores for hate crimes is very close to -1. Project Safe Childhood is less obviously positive or negative by \n",
    "# looking at the title of the project alone, so a slightly negative sentiment score would make sense. \n",
    "\n",
    "\n",
    "## Link for Project Safe Childhood: https://www.justice.gov/psc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic modeling (25 points)\n",
    "\n",
    "For this question, use the `doj_subset_wscores` data that is restricted to civil rights, hate crimes, and project safe childhood and with the sentiment scores added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the data by removing stopwords, punctuation, and non-alpha words (5 points)\n",
    "\n",
    "A. Write a function that:\n",
    "\n",
    "- Takes in a single raw string in the `contents` column from that dataframe\n",
    "- Does the following preprocessing steps:\n",
    "\n",
    "    - Converts the words to lowercase\n",
    "    - Removes stopwords, adding the custom stopwords in your code cell below to the default stopwords list\n",
    "    - Only retains alpha words (so removes digits and punctuation)\n",
    "    - Only retains words 4 characters or longer\n",
    "    - Uses the snowball stemmer from nltk to stem\n",
    "\n",
    "- Returns a joined preprocessed string\n",
    "    \n",
    "B. Use `apply` or list comprehension to execute that function and create a new column in the data called `processed_text`\n",
    "    \n",
    "C. Print the `id`, `contents`, and `processed_text` columns for the following press releases:\n",
    "\n",
    "id = 16-718 (this case: https://www.seattletimes.com/nation-world/doj-miami-police-reach-settlement-in-civil-rights-case/)\n",
    "\n",
    "id = 16-217 (this case: https://www.wlbt.com/story/32275512/three-mississippi-correctional-officers-indicted-for-inmate-assault-and-cover-up/)\n",
    "    \n",
    "**Resources**:\n",
    "\n",
    "- Here's code examples for the snowball stemmer: https://www.geeksforgeeks.org/snowball-stemmer-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_4 = doj_subset_wscore[doj_subset_wscore['id'] == '16-718']['contents']\n",
    "\n",
    "pharma_4 = subset_4.str.cat(sep=' ')\n",
    "\n",
    "pharma_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_doj_stopwords = [\"civil\", \"rights\", \"division\", \"department\", \"justice\",\n",
    "                        \"office\", \"attorney\", \"district\", \"case\", \"investigation\", \"assistant\",\n",
    "                       \"trial\", \"assistance\", \"assist\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A: writing a function\n",
    "\n",
    "porter = SnowballStemmer(language = \"english\")\n",
    "\n",
    "## convert to lowercase and a list\n",
    "\n",
    "corpus_lower = doj_subset_wscore.contents.str.lower().to_list()\n",
    "\n",
    "## use wordpunct tokenize and filter out with one\n",
    "\n",
    "def process_step1(text):   \n",
    "    \n",
    "    stop_words = set(stopwords.words('english')).union(custom_doj_stopwords)\n",
    "\n",
    "    nostop_listing1 = [word for word in wordpunct_tokenize(text)\n",
    "                      if word not in stop_words]\n",
    "    \n",
    "    processed = [porter.stem(word) for word in nostop_listing1\n",
    "                    if word.isalpha() \n",
    "                    and len(word) >= 4]\n",
    "    \n",
    "    processed_str = \" \".join(processed)\n",
    "    \n",
    "    return(processed_str)\n",
    "\n",
    "results_2 = [process_step1(text) for text in doj_subset_wscore['contents']]\n",
    "\n",
    "results_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B: applying and creating a new column \n",
    "\n",
    "doj_subset_wscore['processed_text'] = [process_step1(content) for content in doj_subset_wscore['contents']]\n",
    "\n",
    "doj_subset_wscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## C: printing id, contents, and processed_text for examples \n",
    "\n",
    "row_16718 = doj_subset_wscore[doj_subset_wscore[\"id\"] == \"16-718\"]\n",
    "row_16217 = doj_subset_wscore[doj_subset_wscore[\"id\"] == \"16-217\"]\n",
    "\n",
    "print(row_16718[['id', 'contents', 'processed_text']])\n",
    "print(row_16217[['id', 'contents', 'processed_text']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create a document-term matrix from the preprocessed press releases and to explore top words (5 points)\n",
    "\n",
    "A. Use the `create_dtm` function I provide (alternately, feel free to write your own!) and create a document-term matrix using the preprocessed press releases; make sure metadata contains the following columns: `id`, `compound` sentiment column you added, and the `topics_clean` column\n",
    "\n",
    "B. Print the top 10 words for press releases with compound sentiment in the top 5% (so the most positive sentiment)\n",
    "\n",
    "C. Print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\n",
    "\n",
    "**Hint**: for these, remember the pandas quantile function from pset one.  \n",
    "\n",
    "D. Print the top 10 words for press releases in each of the three `topics_clean`\n",
    "\n",
    "For steps B - D, to receive full credit, write a function `get_topwords` that helps you avoid duplicated code when you find top words for the different subsets of the data. There are different ways to structure it but one way is to feed it subsetted data (so data subsetted to one topic etc.) and for it to get the top words for that subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), \n",
    "        columns=vectorizer.get_feature_names_out())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: create_dtm \n",
    "\n",
    "dtm_nopre = create_dtm(list_of_strings = doj_subset_wscore[\"processed_text\"],\n",
    "                metadata = doj_subset_wscore[['id', 'compound_score', 'topics_clean']])\n",
    "\n",
    "dtm_nopre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B: print the top 10 words for press releases with compound sentiment in the top 5% (so the most positive sentiment)\n",
    "\n",
    "## creating get_topwords\n",
    "\n",
    "def get_topwords(dtm, num_words = 10):\n",
    "    \n",
    "    word_counts = dtm_nopre[dtm.columns[4:]].sum(axis=0).sort_values(ascending = False)\n",
    "    \n",
    "    return word_counts.head(num_words)\n",
    "\n",
    "# threshold for top 5% of scores\n",
    "top_5 = dtm_nopre['compound_score'].quantile(0.95)\n",
    "\n",
    "# subsets of compounds\n",
    "five_percent = dtm_nopre[dtm_nopre['compound_score'] >= top_5]\n",
    "\n",
    "# print statements \n",
    "print(\"Top 10 words for press releases with compount sentiment in the top 5%\", get_topwords(five_percent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[313], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## C: print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m low_5 \u001b[38;5;241m=\u001b[39m dtm_nopre[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m five_below \u001b[38;5;241m=\u001b[39m dtm_nopre[dtm_nopre[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m low_5]\n\u001b[1;32m      7\u001b[0m five_below \u001b[38;5;241m=\u001b[39m five_below\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m      9\u001b[0m word_counts_2 \u001b[38;5;241m=\u001b[39m five_below\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_clean\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3880\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3878\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) DataFrame?\u001b[39;00m\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame):\n\u001b[0;32m-> 3880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(key)\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:10615\u001b[0m, in \u001b[0;36mNDFrame.where\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m  10609\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  10610\u001b[0m                 _chained_assignment_method_msg,\n\u001b[1;32m  10611\u001b[0m                 ChainedAssignmentError,\n\u001b[1;32m  10612\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m  10613\u001b[0m             )\n\u001b[1;32m  10614\u001b[0m other \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mapply_if_callable(other, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m> 10615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_where(cond, other, inplace, axis, level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:10333\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[0;34m(self, cond, other, inplace, axis, level)\u001b[0m\n\u001b[1;32m  10330\u001b[0m     cond \u001b[38;5;241m=\u001b[39m cond\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m  10332\u001b[0m cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mcond \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m cond\n\u001b[0;32m> 10333\u001b[0m cond \u001b[38;5;241m=\u001b[39m cond\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis_number, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m  10335\u001b[0m \u001b[38;5;66;03m# try to align with other\u001b[39;00m\n\u001b[1;32m  10336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, NDFrame):\n\u001b[1;32m  10337\u001b[0m     \u001b[38;5;66;03m# align with me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5141\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5122\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5123\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,\n\u001b[1;32m   5124\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5139\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m-> 5141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[1;32m   5142\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5143\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5144\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5145\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5146\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m   5147\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5148\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5149\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5150\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[1;32m   5151\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m   5152\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:5521\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5520\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_axes(\n\u001b[1;32m   5522\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[1;32m   5523\u001b[0m )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:5544\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5541\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5543\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5544\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[1;32m   5545\u001b[0m     labels, level\u001b[38;5;241m=\u001b[39mlevel, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance, method\u001b[38;5;241m=\u001b[39mmethod\n\u001b[1;32m   5546\u001b[0m )\n\u001b[1;32m   5548\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5549\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5550\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5551\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5552\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5553\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5554\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:4434\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   4433\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[0;32m-> 4434\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4436\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "## C: print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\n",
    "\n",
    "low_5 = dtm_nopre['compound'].quantile(0.05)\n",
    "\n",
    "five_below = dtm_nopre[dtm_nopre['compound'] <= low_5]\n",
    "\n",
    "five_below = five_below.drop_duplicates()\n",
    "\n",
    "word_counts_2 = five_below.drop(columns = ['id', 'compound', 'topics_clean']).sum()\n",
    "\n",
    "low_10 = word_counts_2.sort_values(ascending = False).head(10)\n",
    "\n",
    "print(\"Top 10 words for press releases with compound sentiment in the top 5%\", low_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 8\n",
      "e: 6\n",
      "c: 6\n",
      "d: 4\n",
      "t: 4\n",
      "o: 4\n",
      "s: 4\n",
      "h: 4\n",
      "l: 3\n",
      "a: 3\n",
      "\n",
      "i: 8\n",
      "e: 6\n",
      "c: 6\n",
      "d: 4\n",
      "t: 4\n",
      "o: 4\n",
      "s: 4\n",
      "h: 4\n",
      "l: 3\n",
      "a: 3\n",
      "\n",
      "i: 8\n",
      "e: 6\n",
      "c: 6\n",
      "d: 4\n",
      "t: 4\n",
      "o: 4\n",
      "s: 4\n",
      "h: 4\n",
      "l: 3\n",
      "a: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## D: Print the top 10 words for press releases in each of the three `topics_clean`\n",
    "\n",
    "topics = ['Hate Crimes', 'Civil Rights', 'Project Safe Childhood']\n",
    "\n",
    "concat_press = {}\n",
    "\n",
    "for topic in topics:\n",
    "    press_topic = doj_subset_wscore[doj_subset_wscore['topics_clean'] == topic]['contents'].tolist()\n",
    "\n",
    "    concat_press[topic] = ' '.join(press_topic)\n",
    "\n",
    "for topic in topics:    \n",
    "    dtm_press = create_dtm(list_of_strings = concat_press,\n",
    "                            metadata = doj_subset_wscore[['id', 'topics_clean']])\n",
    "\n",
    "    word_counts = {}\n",
    "    for press_release_dtm in dtm_press:\n",
    "        for word in press_release_dtm:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    # Sort words by frequency and print the top 10 words\n",
    "    sorted_word_counts = sorted(word_counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    for word, count in sorted_word_counts[:10]:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Estimate a topic model using those preprocessed words (5 points)\n",
    "\n",
    "A. Going back to the preprocessed words from part 2.3.1, estimate a topic model with 3 topics, since you want to see if the unsupervised topic models recover different themes for each of the three manually-labeled areas (civil rights; hate crimes; project safe childhood). You have free rein over the other topic model parameters beyond the number of topics.\n",
    "\n",
    "B. After estimating the topic model, print the top 15 words in each topic.\n",
    "\n",
    "**Hints and Resources**:\n",
    "\n",
    "- Same topic modeling resources linked to above\n",
    "- Make sure to use the `random_state` argument within the model so that the numbering of topics does not move around between runs of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',', 1: '-', 2: '.', 3: '.,', 4: '10'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out very rare and very common words reduced the length of dictionary from 12590 to 903.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: ',', 1: '-', 2: '.', 3: '.,', 4: '10'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of documents represented in dictionary format (with omitted words noted):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([(0, 17),\n",
       "   (1, 1),\n",
       "   (2, 16),\n",
       "   (3, 1),\n",
       "   (4, 1),\n",
       "   (5, 1),\n",
       "   (6, 1),\n",
       "   (7, 1),\n",
       "   (8, 1),\n",
       "   (9, 1),\n",
       "   (10, 1),\n",
       "   (11, 2),\n",
       "   (12, 1),\n",
       "   (13, 1),\n",
       "   (14, 1),\n",
       "   (15, 1),\n",
       "   (16, 1),\n",
       "   (17, 2),\n",
       "   (18, 2),\n",
       "   (19, 2),\n",
       "   (20, 1),\n",
       "   (21, 1),\n",
       "   (22, 1),\n",
       "   (23, 1),\n",
       "   (24, 2),\n",
       "   (25, 5),\n",
       "   (26, 1),\n",
       "   (27, 1),\n",
       "   (28, 2),\n",
       "   (29, 1),\n",
       "   (30, 1),\n",
       "   (31, 1),\n",
       "   (32, 3),\n",
       "   (33, 1),\n",
       "   (34, 1),\n",
       "   (35, 1),\n",
       "   (36, 6),\n",
       "   (37, 1),\n",
       "   (38, 1),\n",
       "   (39, 2),\n",
       "   (40, 2),\n",
       "   (41, 1),\n",
       "   (42, 1),\n",
       "   (43, 2),\n",
       "   (44, 15),\n",
       "   (45, 1),\n",
       "   (46, 1),\n",
       "   (47, 1),\n",
       "   (48, 2),\n",
       "   (49, 1),\n",
       "   (50, 2),\n",
       "   (51, 1),\n",
       "   (52, 6),\n",
       "   (53, 3),\n",
       "   (54, 1),\n",
       "   (55, 1),\n",
       "   (56, 2),\n",
       "   (57, 1),\n",
       "   (58, 1),\n",
       "   (59, 2),\n",
       "   (60, 1),\n",
       "   (61, 1),\n",
       "   (62, 1),\n",
       "   (63, 1),\n",
       "   (64, 1),\n",
       "   (65, 3),\n",
       "   (66, 1),\n",
       "   (67, 1),\n",
       "   (68, 1),\n",
       "   (69, 1),\n",
       "   (70, 1),\n",
       "   (71, 1),\n",
       "   (72, 1),\n",
       "   (73, 5),\n",
       "   (74, 1),\n",
       "   (75, 5),\n",
       "   (76, 1),\n",
       "   (77, 1),\n",
       "   (78, 1),\n",
       "   (79, 1),\n",
       "   (80, 3),\n",
       "   (81, 1),\n",
       "   (82, 1),\n",
       "   (83, 1),\n",
       "   (84, 9),\n",
       "   (85, 1),\n",
       "   (86, 4),\n",
       "   (87, 3),\n",
       "   (88, 1),\n",
       "   (89, 3),\n",
       "   (90, 1),\n",
       "   (91, 1),\n",
       "   (92, 1),\n",
       "   (93, 1),\n",
       "   (94, 1),\n",
       "   (95, 1),\n",
       "   (96, 1),\n",
       "   (97, 2),\n",
       "   (98, 1),\n",
       "   (99, 1),\n",
       "   (100, 4),\n",
       "   (101, 17),\n",
       "   (102, 1),\n",
       "   (103, 3),\n",
       "   (104, 9),\n",
       "   (105, 1),\n",
       "   (106, 1),\n",
       "   (107, 1),\n",
       "   (108, 1),\n",
       "   (109, 4),\n",
       "   (110, 1),\n",
       "   (111, 2),\n",
       "   (112, 1),\n",
       "   (113, 4),\n",
       "   (114, 2),\n",
       "   (115, 1),\n",
       "   (116, 7),\n",
       "   (117, 3),\n",
       "   (118, 2)],\n",
       "  {'47': 1,\n",
       "   'Coleman': 1,\n",
       "   'Corrections': 1,\n",
       "   'Deonte': 1,\n",
       "   'Each': 1,\n",
       "   'Indictment': 1,\n",
       "   'Lawardrick': 1,\n",
       "   'Marsher': 6,\n",
       "   'Mulhauser': 1,\n",
       "   'Parchman': 1,\n",
       "   'Pate': 1,\n",
       "   'Penitentiary': 1,\n",
       "   'Sturdivant': 5,\n",
       "   'beating': 4,\n",
       "   'correctional': 1,\n",
       "   'cruel': 1,\n",
       "   'failing': 1,\n",
       "   'helping': 1,\n",
       "   'inmate': 1,\n",
       "   'intervene': 1,\n",
       "   'kicking': 1,\n",
       "   'lied': 1,\n",
       "   'nine': 1,\n",
       "   'prisoner': 1,\n",
       "   'punching': 2,\n",
       "   'submitted': 1,\n",
       "   'third': 2,\n",
       "   'throwing': 1,\n",
       "   'unsealed': 1,\n",
       "   'unusual': 1,\n",
       "   'weapon': 1}),\n",
       " ([(0, 12),\n",
       "   (1, 2),\n",
       "   (2, 18),\n",
       "   (3, 1),\n",
       "   (8, 1),\n",
       "   (10, 1),\n",
       "   (11, 3),\n",
       "   (12, 1),\n",
       "   (15, 2),\n",
       "   (16, 1),\n",
       "   (17, 2),\n",
       "   (18, 2),\n",
       "   (20, 1),\n",
       "   (27, 1),\n",
       "   (29, 3),\n",
       "   (32, 1),\n",
       "   (33, 2),\n",
       "   (34, 1),\n",
       "   (35, 3),\n",
       "   (36, 5),\n",
       "   (44, 8),\n",
       "   (48, 1),\n",
       "   (50, 4),\n",
       "   (52, 1),\n",
       "   (56, 1),\n",
       "   (64, 1),\n",
       "   (67, 1),\n",
       "   (72, 1),\n",
       "   (73, 5),\n",
       "   (77, 1),\n",
       "   (78, 1),\n",
       "   (80, 3),\n",
       "   (82, 1),\n",
       "   (83, 1),\n",
       "   (84, 10),\n",
       "   (87, 5),\n",
       "   (88, 1),\n",
       "   (89, 1),\n",
       "   (90, 1),\n",
       "   (91, 1),\n",
       "   (97, 4),\n",
       "   (100, 2),\n",
       "   (101, 13),\n",
       "   (104, 2),\n",
       "   (107, 1),\n",
       "   (111, 2),\n",
       "   (113, 2),\n",
       "   (116, 3),\n",
       "   (118, 4),\n",
       "   (119, 3),\n",
       "   (120, 1),\n",
       "   (121, 1),\n",
       "   (122, 1),\n",
       "   (123, 1),\n",
       "   (124, 1),\n",
       "   (125, 1),\n",
       "   (126, 1),\n",
       "   (127, 2),\n",
       "   (128, 1),\n",
       "   (129, 1),\n",
       "   (130, 1),\n",
       "   (131, 1),\n",
       "   (132, 1),\n",
       "   (133, 1),\n",
       "   (134, 1),\n",
       "   (135, 1),\n",
       "   (136, 1),\n",
       "   (137, 1),\n",
       "   (138, 1),\n",
       "   (139, 2),\n",
       "   (140, 1),\n",
       "   (141, 1),\n",
       "   (142, 1),\n",
       "   (143, 1),\n",
       "   (144, 1),\n",
       "   (145, 1),\n",
       "   (146, 1),\n",
       "   (147, 5),\n",
       "   (148, 1),\n",
       "   (149, 1),\n",
       "   (150, 5),\n",
       "   (151, 1),\n",
       "   (152, 1),\n",
       "   (153, 1),\n",
       "   (154, 3),\n",
       "   (155, 1),\n",
       "   (156, 1),\n",
       "   (157, 2),\n",
       "   (158, 1),\n",
       "   (159, 1),\n",
       "   (160, 1),\n",
       "   (161, 1),\n",
       "   (162, 1),\n",
       "   (163, 1),\n",
       "   (164, 1),\n",
       "   (165, 1),\n",
       "   (166, 1),\n",
       "   (167, 1),\n",
       "   (168, 2),\n",
       "   (169, 1),\n",
       "   (170, 1),\n",
       "   (171, 1),\n",
       "   (172, 1)],\n",
       "  {'58': 1,\n",
       "   'AeJean': 1,\n",
       "   'Albuquerque': 3,\n",
       "   'Baker': 1,\n",
       "   'Bakery': 1,\n",
       "   'Cha': 1,\n",
       "   'Delicatessen': 1,\n",
       "   'Jewish': 2,\n",
       "   'Mexico': 1,\n",
       "   'N': 1,\n",
       "   'Ng': 6,\n",
       "   'Nosh': 1,\n",
       "   'Semitic': 2,\n",
       "   'appearance': 1,\n",
       "   'business': 2,\n",
       "   'establishes': 1,\n",
       "   'federally': 1,\n",
       "   'initial': 1,\n",
       "   'interfered': 1,\n",
       "   'interfering': 1,\n",
       "   'notes': 1,\n",
       "   'offense': 2,\n",
       "   'operates': 1,\n",
       "   'owns': 1,\n",
       "   'posted': 1,\n",
       "   'probable': 1,\n",
       "   'vicinity': 1}),\n",
       " ([(0, 23),\n",
       "   (2, 26),\n",
       "   (5, 2),\n",
       "   (10, 2),\n",
       "   (11, 4),\n",
       "   (12, 3),\n",
       "   (15, 2),\n",
       "   (16, 2),\n",
       "   (17, 4),\n",
       "   (18, 1),\n",
       "   (26, 1),\n",
       "   (27, 3),\n",
       "   (29, 3),\n",
       "   (32, 5),\n",
       "   (33, 2),\n",
       "   (34, 1),\n",
       "   (35, 3),\n",
       "   (36, 11),\n",
       "   (39, 1),\n",
       "   (42, 1),\n",
       "   (43, 1),\n",
       "   (44, 12),\n",
       "   (46, 2),\n",
       "   (47, 2),\n",
       "   (49, 1),\n",
       "   (50, 4),\n",
       "   (51, 2),\n",
       "   (60, 2),\n",
       "   (63, 2),\n",
       "   (67, 2),\n",
       "   (70, 1),\n",
       "   (72, 2),\n",
       "   (73, 8),\n",
       "   (76, 1),\n",
       "   (78, 1),\n",
       "   (79, 1),\n",
       "   (80, 2),\n",
       "   (84, 5),\n",
       "   (87, 5),\n",
       "   (89, 1),\n",
       "   (90, 1),\n",
       "   (97, 3),\n",
       "   (98, 1),\n",
       "   (100, 1),\n",
       "   (101, 19),\n",
       "   (104, 9),\n",
       "   (105, 1),\n",
       "   (111, 6),\n",
       "   (112, 2),\n",
       "   (113, 6),\n",
       "   (115, 1),\n",
       "   (116, 5),\n",
       "   (118, 4),\n",
       "   (122, 1),\n",
       "   (126, 1),\n",
       "   (127, 2),\n",
       "   (138, 1),\n",
       "   (141, 1),\n",
       "   (142, 1),\n",
       "   (152, 2),\n",
       "   (153, 4),\n",
       "   (156, 4),\n",
       "   (165, 1),\n",
       "   (167, 1),\n",
       "   (173, 1),\n",
       "   (174, 1),\n",
       "   (175, 2),\n",
       "   (176, 1),\n",
       "   (177, 2),\n",
       "   (178, 1),\n",
       "   (179, 1),\n",
       "   (180, 3),\n",
       "   (181, 3),\n",
       "   (182, 3),\n",
       "   (183, 2),\n",
       "   (184, 1),\n",
       "   (185, 1),\n",
       "   (186, 1),\n",
       "   (187, 3),\n",
       "   (188, 3),\n",
       "   (189, 3),\n",
       "   (190, 1),\n",
       "   (191, 3),\n",
       "   (192, 1),\n",
       "   (193, 1),\n",
       "   (194, 3),\n",
       "   (195, 1),\n",
       "   (196, 3),\n",
       "   (197, 3),\n",
       "   (198, 1),\n",
       "   (199, 1),\n",
       "   (200, 1),\n",
       "   (201, 1),\n",
       "   (202, 1),\n",
       "   (203, 3),\n",
       "   (204, 1),\n",
       "   (205, 1),\n",
       "   (206, 2),\n",
       "   (207, 1),\n",
       "   (208, 1),\n",
       "   (209, 1),\n",
       "   (210, 1),\n",
       "   (211, 1),\n",
       "   (212, 1),\n",
       "   (213, 2),\n",
       "   (214, 1),\n",
       "   (215, 1),\n",
       "   (216, 1),\n",
       "   (217, 1),\n",
       "   (218, 1),\n",
       "   (219, 1),\n",
       "   (220, 1),\n",
       "   (221, 1),\n",
       "   (222, 1),\n",
       "   (223, 1),\n",
       "   (224, 1),\n",
       "   (225, 1),\n",
       "   (226, 1),\n",
       "   (227, 1),\n",
       "   (228, 1),\n",
       "   (229, 1),\n",
       "   (230, 1),\n",
       "   (231, 1),\n",
       "   (232, 1),\n",
       "   (233, 2),\n",
       "   (234, 1),\n",
       "   (235, 1),\n",
       "   (236, 1),\n",
       "   (237, 2),\n",
       "   (238, 2),\n",
       "   (239, 1),\n",
       "   (240, 2),\n",
       "   (241, 1),\n",
       "   (242, 1),\n",
       "   (243, 1),\n",
       "   (244, 1),\n",
       "   (245, 1),\n",
       "   (246, 1),\n",
       "   (247, 1),\n",
       "   (248, 1),\n",
       "   (249, 1),\n",
       "   (250, 3)],\n",
       "  {'71': 1,\n",
       "   'Adriana': 1,\n",
       "   'Aryan': 1,\n",
       "   'Brotherhood': 1,\n",
       "   'Brutality': 1,\n",
       "   'Connor': 1,\n",
       "   'Correctional': 1,\n",
       "   'Dallas': 1,\n",
       "   'Errin': 1,\n",
       "   'FCI': 2,\n",
       "   'Hall': 7,\n",
       "   'Institution': 1,\n",
       "   'Martin': 1,\n",
       "   'North': 1,\n",
       "   'O': 1,\n",
       "   'Reed': 1,\n",
       "   'Saldaña': 1,\n",
       "   'Sarah': 1,\n",
       "   'Seagoville': 2,\n",
       "   'Vieco': 1,\n",
       "   'arsenal': 1,\n",
       "   'attacked': 1,\n",
       "   'attorneys': 1,\n",
       "   'civilized': 1,\n",
       "   'consciousness': 1,\n",
       "   'consecutively': 1,\n",
       "   'currently': 1,\n",
       "   'eye': 1,\n",
       "   'feet': 1,\n",
       "   'fellow': 3,\n",
       "   'fractured': 2,\n",
       "   'gay': 2,\n",
       "   'homophobic': 1,\n",
       "   'hospital': 1,\n",
       "   'inmate': 5,\n",
       "   'inside': 1,\n",
       "   'kicked': 1,\n",
       "   'lacerations': 1,\n",
       "   'lost': 2,\n",
       "   'male': 1,\n",
       "   'orientation': 1,\n",
       "   'partnership': 1,\n",
       "   'pleading': 1,\n",
       "   'prioritize': 1,\n",
       "   'relationship': 1,\n",
       "   'sends': 1,\n",
       "   'served': 1,\n",
       "   'serving': 1,\n",
       "   'shod': 1,\n",
       "   'slur': 1,\n",
       "   'socket': 1,\n",
       "   'stemming': 1,\n",
       "   'stomped': 1,\n",
       "   'sustained': 2,\n",
       "   'teeth': 1,\n",
       "   'tools': 1,\n",
       "   'tooth': 1,\n",
       "   'treated': 1,\n",
       "   'unprovoked': 1,\n",
       "   'weapon': 2,\n",
       "   'yelling': 1}),\n",
       " ([(0, 18),\n",
       "   (1, 5),\n",
       "   (2, 20),\n",
       "   (3, 1),\n",
       "   (5, 1),\n",
       "   (10, 2),\n",
       "   (11, 4),\n",
       "   (12, 2),\n",
       "   (15, 3),\n",
       "   (16, 3),\n",
       "   (17, 3),\n",
       "   (18, 1),\n",
       "   (20, 1),\n",
       "   (22, 2),\n",
       "   (27, 2),\n",
       "   (29, 2),\n",
       "   (31, 1),\n",
       "   (32, 4),\n",
       "   (33, 2),\n",
       "   (34, 1),\n",
       "   (35, 2),\n",
       "   (36, 12),\n",
       "   (38, 1),\n",
       "   (44, 15),\n",
       "   (45, 1),\n",
       "   (46, 2),\n",
       "   (48, 2),\n",
       "   (49, 1),\n",
       "   (50, 3),\n",
       "   (51, 2),\n",
       "   (54, 1),\n",
       "   (55, 1),\n",
       "   (56, 2),\n",
       "   (57, 1),\n",
       "   (58, 2),\n",
       "   (64, 1),\n",
       "   (66, 1),\n",
       "   (67, 5),\n",
       "   (72, 1),\n",
       "   (73, 9),\n",
       "   (76, 1),\n",
       "   (78, 1),\n",
       "   (80, 6),\n",
       "   (82, 1),\n",
       "   (84, 18),\n",
       "   (89, 1),\n",
       "   (90, 1),\n",
       "   (92, 1),\n",
       "   (97, 5),\n",
       "   (100, 5),\n",
       "   (101, 28),\n",
       "   (102, 1),\n",
       "   (104, 11),\n",
       "   (105, 2),\n",
       "   (109, 1),\n",
       "   (110, 1),\n",
       "   (111, 1),\n",
       "   (113, 2),\n",
       "   (114, 1),\n",
       "   (116, 1),\n",
       "   (117, 1),\n",
       "   (118, 7),\n",
       "   (127, 2),\n",
       "   (132, 1),\n",
       "   (138, 1),\n",
       "   (142, 1),\n",
       "   (148, 1),\n",
       "   (149, 3),\n",
       "   (151, 3),\n",
       "   (152, 5),\n",
       "   (153, 1),\n",
       "   (159, 1),\n",
       "   (160, 3),\n",
       "   (168, 1),\n",
       "   (170, 2),\n",
       "   (175, 2),\n",
       "   (176, 1),\n",
       "   (177, 1),\n",
       "   (180, 1),\n",
       "   (181, 2),\n",
       "   (182, 1),\n",
       "   (184, 1),\n",
       "   (186, 1),\n",
       "   (187, 1),\n",
       "   (188, 2),\n",
       "   (189, 2),\n",
       "   (191, 2),\n",
       "   (193, 1),\n",
       "   (194, 1),\n",
       "   (196, 2),\n",
       "   (198, 1),\n",
       "   (199, 1),\n",
       "   (202, 1),\n",
       "   (210, 2),\n",
       "   (211, 2),\n",
       "   (212, 1),\n",
       "   (213, 1),\n",
       "   (216, 1),\n",
       "   (217, 1),\n",
       "   (218, 1),\n",
       "   (222, 1),\n",
       "   (232, 1),\n",
       "   (233, 1),\n",
       "   (237, 2),\n",
       "   (242, 1),\n",
       "   (246, 1),\n",
       "   (249, 1),\n",
       "   (250, 4),\n",
       "   (251, 1),\n",
       "   (252, 1),\n",
       "   (253, 1),\n",
       "   (254, 1),\n",
       "   (255, 1),\n",
       "   (256, 1),\n",
       "   (257, 1),\n",
       "   (258, 1),\n",
       "   (259, 1),\n",
       "   (260, 1),\n",
       "   (261, 1),\n",
       "   (262, 1),\n",
       "   (263, 1),\n",
       "   (264, 3),\n",
       "   (265, 1),\n",
       "   (266, 1),\n",
       "   (267, 1),\n",
       "   (268, 1),\n",
       "   (269, 4),\n",
       "   (270, 1),\n",
       "   (271, 1),\n",
       "   (272, 3),\n",
       "   (273, 2),\n",
       "   (274, 1),\n",
       "   (275, 1),\n",
       "   (276, 1),\n",
       "   (277, 1),\n",
       "   (278, 2),\n",
       "   (279, 1),\n",
       "   (280, 1),\n",
       "   (281, 1),\n",
       "   (282, 2),\n",
       "   (283, 1),\n",
       "   (284, 1),\n",
       "   (285, 1),\n",
       "   (286, 1),\n",
       "   (287, 1),\n",
       "   (288, 1),\n",
       "   (289, 3),\n",
       "   (290, 1),\n",
       "   (291, 1),\n",
       "   (292, 1),\n",
       "   (293, 1),\n",
       "   (294, 1),\n",
       "   (295, 1),\n",
       "   (296, 1),\n",
       "   (297, 1),\n",
       "   (298, 1),\n",
       "   (299, 2),\n",
       "   (300, 1),\n",
       "   (301, 1),\n",
       "   (302, 1),\n",
       "   (303, 1),\n",
       "   (304, 1),\n",
       "   (305, 2),\n",
       "   (306, 1),\n",
       "   (307, 7),\n",
       "   (308, 1),\n",
       "   (309, 1),\n",
       "   (310, 2),\n",
       "   (311, 1)],\n",
       "  {'Ark': 1,\n",
       "   'Arkansas': 4,\n",
       "   'Carroll': 1,\n",
       "   'Chung': 1,\n",
       "   'Conner': 1,\n",
       "   'Edward': 1,\n",
       "   'Eldridge': 1,\n",
       "   'Fayetteville': 1,\n",
       "   'Forest': 1,\n",
       "   'Green': 1,\n",
       "   'Hispanic': 1,\n",
       "   'Information': 1,\n",
       "   'Jenner': 1,\n",
       "   'Kyra': 1,\n",
       "   'Popejoy': 3,\n",
       "   'Sean': 1,\n",
       "   'ago': 1,\n",
       "   'brutally': 1,\n",
       "   'car': 2,\n",
       "   'caught': 1,\n",
       "   'conspirator': 1,\n",
       "   'continues': 1,\n",
       "   'crash': 1,\n",
       "   'decade': 1,\n",
       "   'disturbing': 1,\n",
       "   'enacted': 2,\n",
       "   'epithets': 1,\n",
       "   'gas': 1,\n",
       "   'go': 1,\n",
       "   'hurl': 1,\n",
       "   'ignite': 1,\n",
       "   'injure': 1,\n",
       "   'lane': 1,\n",
       "   'leaned': 1,\n",
       "   'lot': 1,\n",
       "   'murdered': 1,\n",
       "   'names': 1,\n",
       "   'occur': 3,\n",
       "   'off': 1,\n",
       "   'opposite': 1,\n",
       "   'parking': 1,\n",
       "   'passenger': 1,\n",
       "   'pulled': 1,\n",
       "   'pursued': 1,\n",
       "   'rammed': 1,\n",
       "   'road': 1,\n",
       "   'someone': 1,\n",
       "   'station': 1,\n",
       "   'sustained': 1,\n",
       "   'terrible': 1,\n",
       "   'tire': 1,\n",
       "   'tool': 1,\n",
       "   'traffic': 1,\n",
       "   'tree': 1,\n",
       "   'truck': 1,\n",
       "   'unacceptable': 1,\n",
       "   'waived': 1,\n",
       "   'whenever': 1,\n",
       "   'wherever': 1,\n",
       "   'window': 1,\n",
       "   'wrench': 1}),\n",
       " ([(0, 18),\n",
       "   (1, 4),\n",
       "   (2, 24),\n",
       "   (3, 1),\n",
       "   (8, 1),\n",
       "   (10, 2),\n",
       "   (11, 4),\n",
       "   (12, 2),\n",
       "   (13, 1),\n",
       "   (16, 2),\n",
       "   (17, 3),\n",
       "   (18, 1),\n",
       "   (21, 2),\n",
       "   (26, 2),\n",
       "   (27, 2),\n",
       "   (29, 6),\n",
       "   (30, 1),\n",
       "   (32, 3),\n",
       "   (33, 1),\n",
       "   (35, 3),\n",
       "   (36, 5),\n",
       "   (37, 1),\n",
       "   (39, 1),\n",
       "   (42, 1),\n",
       "   (43, 1),\n",
       "   (44, 10),\n",
       "   (45, 2),\n",
       "   (48, 2),\n",
       "   (49, 2),\n",
       "   (50, 4),\n",
       "   (51, 1),\n",
       "   (53, 3),\n",
       "   (60, 1),\n",
       "   (61, 3),\n",
       "   (67, 4),\n",
       "   (72, 1),\n",
       "   (73, 5),\n",
       "   (76, 2),\n",
       "   (77, 1),\n",
       "   (78, 1),\n",
       "   (80, 4),\n",
       "   (82, 1),\n",
       "   (83, 1),\n",
       "   (84, 14),\n",
       "   (87, 1),\n",
       "   (88, 1),\n",
       "   (89, 1),\n",
       "   (90, 1),\n",
       "   (91, 1),\n",
       "   (97, 3),\n",
       "   (100, 2),\n",
       "   (101, 16),\n",
       "   (102, 1),\n",
       "   (104, 7),\n",
       "   (105, 2),\n",
       "   (108, 1),\n",
       "   (110, 1),\n",
       "   (114, 3),\n",
       "   (115, 1),\n",
       "   (116, 3),\n",
       "   (118, 3),\n",
       "   (123, 1),\n",
       "   (128, 3),\n",
       "   (132, 1),\n",
       "   (135, 1),\n",
       "   (138, 1),\n",
       "   (140, 1),\n",
       "   (141, 1),\n",
       "   (142, 1),\n",
       "   (147, 2),\n",
       "   (148, 1),\n",
       "   (150, 2),\n",
       "   (161, 1),\n",
       "   (164, 2),\n",
       "   (170, 1),\n",
       "   (180, 1),\n",
       "   (181, 1),\n",
       "   (182, 1),\n",
       "   (184, 1),\n",
       "   (186, 1),\n",
       "   (187, 1),\n",
       "   (188, 1),\n",
       "   (191, 1),\n",
       "   (193, 1),\n",
       "   (194, 1),\n",
       "   (196, 1),\n",
       "   (198, 1),\n",
       "   (207, 2),\n",
       "   (210, 1),\n",
       "   (222, 1),\n",
       "   (223, 1),\n",
       "   (227, 2),\n",
       "   (229, 1),\n",
       "   (245, 1),\n",
       "   (248, 1),\n",
       "   (270, 1),\n",
       "   (290, 1),\n",
       "   (291, 3),\n",
       "   (304, 2),\n",
       "   (308, 1),\n",
       "   (311, 1),\n",
       "   (312, 1),\n",
       "   (313, 1),\n",
       "   (314, 1),\n",
       "   (315, 1),\n",
       "   (316, 1),\n",
       "   (317, 1),\n",
       "   (318, 1),\n",
       "   (319, 1),\n",
       "   (320, 1),\n",
       "   (321, 1),\n",
       "   (322, 1),\n",
       "   (323, 2),\n",
       "   (324, 1),\n",
       "   (325, 2),\n",
       "   (326, 2),\n",
       "   (327, 1),\n",
       "   (328, 1),\n",
       "   (329, 1),\n",
       "   (330, 2),\n",
       "   (331, 1),\n",
       "   (332, 1),\n",
       "   (333, 1),\n",
       "   (334, 2),\n",
       "   (335, 1),\n",
       "   (336, 2)],\n",
       "  {'Amish': 3,\n",
       "   'Bergholz': 1,\n",
       "   'Brennan': 1,\n",
       "   'Bridget': 1,\n",
       "   'CLEVELAND': 1,\n",
       "   'Cleveland': 2,\n",
       "   'Dettelbach': 1,\n",
       "   'Eli': 1,\n",
       "   'Emanuel': 1,\n",
       "   'F': 1,\n",
       "   'Hammondsville': 1,\n",
       "   'Johnny': 1,\n",
       "   'Kristy': 1,\n",
       "   'Lester': 1,\n",
       "   'Levi': 1,\n",
       "   'Miller': 2,\n",
       "   'Mullet': 4,\n",
       "   'Ohio': 5,\n",
       "   'Parker': 1,\n",
       "   'Samuel': 1,\n",
       "   'Schrock': 1,\n",
       "   'Seven': 1,\n",
       "   'Sr': 1,\n",
       "   'actual': 1,\n",
       "   'affidavit': 1,\n",
       "   'arrests': 1,\n",
       "   'assaults': 2,\n",
       "   'attacks': 1,\n",
       "   'battery': 1,\n",
       "   'beard': 1,\n",
       "   'beards': 1,\n",
       "   'beyond': 1,\n",
       "   'clippers': 1,\n",
       "   'cut': 1,\n",
       "   'dispute': 1,\n",
       "   'doing': 1,\n",
       "   'doubt': 1,\n",
       "   'fellow': 1,\n",
       "   'forcibly': 1,\n",
       "   'hair': 2,\n",
       "   'having': 1,\n",
       "   'injuring': 1,\n",
       "   'off': 1,\n",
       "   'perceived': 1,\n",
       "   'powered': 1,\n",
       "   'religiously': 2,\n",
       "   'restrained': 1,\n",
       "   'sacred': 1,\n",
       "   'scissors': 1,\n",
       "   'series': 1,\n",
       "   'stop': 1,\n",
       "   'warrants': 1,\n",
       "   'weapon': 1,\n",
       "   'willfully': 1}),\n",
       " ([(0, 17),\n",
       "   (1, 4),\n",
       "   (2, 12),\n",
       "   (4, 3),\n",
       "   (5, 1),\n",
       "   (9, 1),\n",
       "   (10, 1),\n",
       "   (11, 2),\n",
       "   (12, 1),\n",
       "   (13, 1),\n",
       "   (15, 1),\n",
       "   (16, 1),\n",
       "   (17, 1),\n",
       "   (18, 1),\n",
       "   (27, 1),\n",
       "   (29, 1),\n",
       "   (30, 1),\n",
       "   (31, 1),\n",
       "   (32, 4),\n",
       "   (34, 1),\n",
       "   (35, 1),\n",
       "   (36, 7),\n",
       "   (37, 1),\n",
       "   (42, 1),\n",
       "   (43, 2),\n",
       "   (44, 11),\n",
       "   (45, 2),\n",
       "   (46, 2),\n",
       "   (48, 3),\n",
       "   (49, 1),\n",
       "   (50, 4),\n",
       "   (51, 2),\n",
       "   (52, 2),\n",
       "   (53, 5),\n",
       "   (58, 1),\n",
       "   (59, 1),\n",
       "   (61, 2),\n",
       "   (62, 1),\n",
       "   (63, 2),\n",
       "   (64, 1),\n",
       "   (67, 5),\n",
       "   (68, 1),\n",
       "   (72, 1),\n",
       "   (73, 11),\n",
       "   (75, 4),\n",
       "   (76, 1),\n",
       "   (77, 1),\n",
       "   (78, 1),\n",
       "   (80, 4),\n",
       "   (82, 3),\n",
       "   (83, 1),\n",
       "   (84, 16),\n",
       "   (87, 1),\n",
       "   (88, 1),\n",
       "   (89, 3),\n",
       "   (90, 1),\n",
       "   (91, 1),\n",
       "   (95, 1),\n",
       "   (97, 4),\n",
       "   (100, 4),\n",
       "   (101, 12),\n",
       "   (104, 6),\n",
       "   (105, 1),\n",
       "   (106, 2),\n",
       "   (107, 1),\n",
       "   (109, 1),\n",
       "   (113, 4),\n",
       "   (114, 1),\n",
       "   (116, 8),\n",
       "   (117, 2),\n",
       "   (118, 4),\n",
       "   (127, 1),\n",
       "   (135, 1),\n",
       "   (138, 1),\n",
       "   (151, 3),\n",
       "   (153, 2),\n",
       "   (155, 1),\n",
       "   (156, 2),\n",
       "   (157, 1),\n",
       "   (160, 2),\n",
       "   (161, 3),\n",
       "   (163, 1),\n",
       "   (165, 3),\n",
       "   (175, 1),\n",
       "   (176, 1),\n",
       "   (200, 1),\n",
       "   (207, 1),\n",
       "   (208, 3),\n",
       "   (215, 1),\n",
       "   (244, 1),\n",
       "   (245, 1),\n",
       "   (247, 1),\n",
       "   (250, 2),\n",
       "   (260, 1),\n",
       "   (290, 1),\n",
       "   (310, 1),\n",
       "   (321, 1),\n",
       "   (331, 1),\n",
       "   (337, 1),\n",
       "   (338, 2),\n",
       "   (339, 1),\n",
       "   (340, 1),\n",
       "   (341, 1),\n",
       "   (342, 1),\n",
       "   (343, 1),\n",
       "   (344, 1),\n",
       "   (345, 1),\n",
       "   (346, 1),\n",
       "   (347, 1),\n",
       "   (348, 1),\n",
       "   (349, 1),\n",
       "   (350, 1),\n",
       "   (351, 1),\n",
       "   (352, 1),\n",
       "   (353, 1),\n",
       "   (354, 1),\n",
       "   (355, 1),\n",
       "   (356, 1),\n",
       "   (357, 1),\n",
       "   (358, 2),\n",
       "   (359, 1),\n",
       "   (360, 1),\n",
       "   (361, 1),\n",
       "   (362, 1)],\n",
       "  {'38': 1,\n",
       "   '59': 1,\n",
       "   'Count': 2,\n",
       "   'Damon': 1,\n",
       "   'Hawkins': 1,\n",
       "   'Hickman': 8,\n",
       "   'Howell': 5,\n",
       "   'Hydee': 1,\n",
       "   'Indictment': 1,\n",
       "   'Jail': 1,\n",
       "   'Kentucky': 4,\n",
       "   'Larry': 1,\n",
       "   'London': 2,\n",
       "   'Patel': 1,\n",
       "   'Regional': 1,\n",
       "   'Resident': 1,\n",
       "   'River': 1,\n",
       "   'Sanjay': 1,\n",
       "   'Trent': 11,\n",
       "   'acting': 1,\n",
       "   'additionally': 1,\n",
       "   'death': 5,\n",
       "   'deliberate': 1,\n",
       "   'depriving': 1,\n",
       "   'deputy': 1,\n",
       "   'detainee': 1,\n",
       "   'distress': 1,\n",
       "   'fact': 1,\n",
       "   'failing': 1,\n",
       "   'falsification': 1,\n",
       "   'falsifying': 1,\n",
       "   'indicating': 1,\n",
       "   'indifference': 1,\n",
       "   'injured': 1,\n",
       "   'involvement': 1,\n",
       "   'jailers': 1,\n",
       "   'log': 1,\n",
       "   'meaning': 1,\n",
       "   'medical': 1,\n",
       "   'observations': 1,\n",
       "   'obvious': 1,\n",
       "   'offense': 1,\n",
       "   'official': 1,\n",
       "   'pretrial': 1,\n",
       "   'records': 1,\n",
       "   'resulting': 2,\n",
       "   'risk': 1,\n",
       "   'safe': 1,\n",
       "   'thereby': 1}),\n",
       " ([(0, 36),\n",
       "   (1, 10),\n",
       "   (2, 29),\n",
       "   (3, 2),\n",
       "   (4, 2),\n",
       "   (10, 2),\n",
       "   (11, 3),\n",
       "   (12, 2),\n",
       "   (15, 2),\n",
       "   (16, 1),\n",
       "   (17, 2),\n",
       "   (18, 2),\n",
       "   (27, 2),\n",
       "   (29, 5),\n",
       "   (32, 4),\n",
       "   (33, 1),\n",
       "   (34, 1),\n",
       "   (35, 5),\n",
       "   (36, 21),\n",
       "   (42, 2),\n",
       "   (43, 2),\n",
       "   (44, 21),\n",
       "   (46, 1),\n",
       "   (47, 2),\n",
       "   (49, 2),\n",
       "   (50, 4),\n",
       "   (51, 2),\n",
       "   (53, 1),\n",
       "   (54, 5),\n",
       "   (55, 1),\n",
       "   (58, 3),\n",
       "   (61, 1),\n",
       "   (67, 8),\n",
       "   (70, 2),\n",
       "   (72, 3),\n",
       "   (73, 9),\n",
       "   (78, 1),\n",
       "   (80, 1),\n",
       "   (82, 1),\n",
       "   (84, 32),\n",
       "   (87, 2),\n",
       "   (89, 3),\n",
       "   (90, 1),\n",
       "   (92, 1),\n",
       "   (97, 4),\n",
       "   (98, 6),\n",
       "   (100, 5),\n",
       "   (101, 23),\n",
       "   (102, 3),\n",
       "   (103, 1),\n",
       "   (104, 20),\n",
       "   (105, 1),\n",
       "   (113, 4),\n",
       "   (114, 1),\n",
       "   (116, 1),\n",
       "   (117, 6),\n",
       "   (118, 4),\n",
       "   (120, 1),\n",
       "   (125, 1),\n",
       "   (127, 1),\n",
       "   (129, 1),\n",
       "   (132, 1),\n",
       "   (138, 1),\n",
       "   (142, 1),\n",
       "   (143, 1),\n",
       "   (149, 3),\n",
       "   (152, 1),\n",
       "   (153, 3),\n",
       "   (156, 1),\n",
       "   (158, 1),\n",
       "   (160, 3),\n",
       "   (167, 3),\n",
       "   (168, 2),\n",
       "   (184, 1),\n",
       "   (186, 1),\n",
       "   (188, 1),\n",
       "   (190, 1),\n",
       "   (193, 1),\n",
       "   (198, 1),\n",
       "   (211, 2),\n",
       "   (213, 1),\n",
       "   (214, 1),\n",
       "   (216, 1),\n",
       "   (218, 1),\n",
       "   (224, 1),\n",
       "   (226, 1),\n",
       "   (231, 1),\n",
       "   (232, 3),\n",
       "   (237, 2),\n",
       "   (238, 2),\n",
       "   (246, 3),\n",
       "   (249, 1),\n",
       "   (252, 1),\n",
       "   (255, 3),\n",
       "   (257, 2),\n",
       "   (261, 2),\n",
       "   (262, 1),\n",
       "   (264, 1),\n",
       "   (266, 4),\n",
       "   (269, 1),\n",
       "   (271, 2),\n",
       "   (278, 1),\n",
       "   (296, 1),\n",
       "   (299, 1),\n",
       "   (304, 1),\n",
       "   (309, 1),\n",
       "   (310, 1),\n",
       "   (312, 3),\n",
       "   (315, 1),\n",
       "   (323, 1),\n",
       "   (336, 1),\n",
       "   (351, 2),\n",
       "   (358, 2),\n",
       "   (363, 4),\n",
       "   (364, 2),\n",
       "   (365, 2),\n",
       "   (366, 2),\n",
       "   (367, 2),\n",
       "   (368, 2),\n",
       "   (369, 1),\n",
       "   (370, 2),\n",
       "   (371, 1),\n",
       "   (372, 1),\n",
       "   (373, 2),\n",
       "   (374, 1),\n",
       "   (375, 2),\n",
       "   (376, 2),\n",
       "   (377, 1),\n",
       "   (378, 1),\n",
       "   (379, 1),\n",
       "   (380, 1),\n",
       "   (381, 2),\n",
       "   (382, 2),\n",
       "   (383, 2),\n",
       "   (384, 2),\n",
       "   (385, 1),\n",
       "   (386, 2),\n",
       "   (387, 1),\n",
       "   (388, 1),\n",
       "   (389, 1),\n",
       "   (390, 1),\n",
       "   (391, 1),\n",
       "   (392, 1),\n",
       "   (393, 1),\n",
       "   (394, 1),\n",
       "   (395, 1),\n",
       "   (396, 1),\n",
       "   (397, 1),\n",
       "   (398, 1),\n",
       "   (399, 1),\n",
       "   (400, 1),\n",
       "   (401, 2),\n",
       "   (402, 1),\n",
       "   (403, 1),\n",
       "   (404, 1),\n",
       "   (405, 2),\n",
       "   (406, 1),\n",
       "   (407, 1),\n",
       "   (408, 1),\n",
       "   (409, 1),\n",
       "   (410, 2),\n",
       "   (411, 2)],\n",
       "  {'29': 1,\n",
       "   '75': 1,\n",
       "   'Allen': 1,\n",
       "   'Although': 1,\n",
       "   'Ark': 1,\n",
       "   'Arkansas': 1,\n",
       "   'Baptist': 1,\n",
       "   'Barack': 1,\n",
       "   'Bells': 1,\n",
       "   'Breen': 1,\n",
       "   'Brownsville': 1,\n",
       "   'Church': 1,\n",
       "   'Cowart': 7,\n",
       "   'Crockett': 2,\n",
       "   'Edward': 1,\n",
       "   'Helena': 1,\n",
       "   'Jonathan': 1,\n",
       "   'Larry': 1,\n",
       "   'Laurenzi': 1,\n",
       "   'Nevertheless': 1,\n",
       "   'Obama': 2,\n",
       "   'Powell': 1,\n",
       "   'President': 2,\n",
       "   'Schlesselman': 4,\n",
       "   'Secret': 2,\n",
       "   'Senator': 2,\n",
       "   'Skrmetti': 1,\n",
       "   'Stanton': 1,\n",
       "   'Tenn': 2,\n",
       "   'Tennessee': 2,\n",
       "   'Thankfully': 1,\n",
       "   'Threats': 1,\n",
       "   'West': 1,\n",
       "   'able': 1,\n",
       "   'acknowledged': 1,\n",
       "   'additionally': 1,\n",
       "   'assassinating': 1,\n",
       "   'attacks': 1,\n",
       "   'barreled': 3,\n",
       "   'bigotry': 1,\n",
       "   'burglarize': 1,\n",
       "   'candidate': 4,\n",
       "   'carried': 1,\n",
       "   'constitutes': 1,\n",
       "   'culminate': 1,\n",
       "   'damage': 1,\n",
       "   'dealer': 1,\n",
       "   'demand': 1,\n",
       "   'demanded': 1,\n",
       "   'diligence': 1,\n",
       "   'discharge': 1,\n",
       "   'dozens': 1,\n",
       "   'eighteen': 1,\n",
       "   'execute': 1,\n",
       "   'extraordinary': 1,\n",
       "   'federally': 1,\n",
       "   'felonies': 1,\n",
       "   'firearm': 3,\n",
       "   'firearms': 1,\n",
       "   'fueled': 1,\n",
       "   'furtherance': 1,\n",
       "   'grave': 1,\n",
       "   'heroic': 1,\n",
       "   'inflict': 2,\n",
       "   'intended': 1,\n",
       "   'intentional': 1,\n",
       "   'interstate': 2,\n",
       "   'intervention': 1,\n",
       "   'kill': 2,\n",
       "   'killing': 1,\n",
       "   'licensed': 1,\n",
       "   'magnitude': 1,\n",
       "   'major': 1,\n",
       "   'minimum': 1,\n",
       "   'murder': 1,\n",
       "   'obtain': 1,\n",
       "   'off': 1,\n",
       "   'penalties': 1,\n",
       "   'possessing': 1,\n",
       "   'presidential': 3,\n",
       "   'real': 1,\n",
       "   'recognize': 1,\n",
       "   'reflects': 1,\n",
       "   'relation': 1,\n",
       "   'sawed': 1,\n",
       "   'scheme': 2,\n",
       "   'severe': 1,\n",
       "   'shooting': 1,\n",
       "   'short': 3,\n",
       "   'shotgun': 4,\n",
       "   'spared': 1,\n",
       "   'specifically': 1,\n",
       "   'spree': 1,\n",
       "   'stiff': 1,\n",
       "   'targeting': 1,\n",
       "   'tragedy': 1,\n",
       "   'transportation': 3,\n",
       "   'transporting': 1,\n",
       "   'twelve': 1,\n",
       "   'unauthorized': 1,\n",
       "   'unlicensed': 1,\n",
       "   'us': 1,\n",
       "   'weapons': 1,\n",
       "   'window': 1}),\n",
       " ([(0, 42),\n",
       "   (1, 2),\n",
       "   (2, 28),\n",
       "   (8, 2),\n",
       "   (9, 1),\n",
       "   (10, 2),\n",
       "   (11, 7),\n",
       "   (12, 2),\n",
       "   (15, 4),\n",
       "   (16, 2),\n",
       "   (17, 3),\n",
       "   (27, 2),\n",
       "   (29, 7),\n",
       "   (32, 5),\n",
       "   (34, 1),\n",
       "   (35, 7),\n",
       "   (36, 6),\n",
       "   (39, 1),\n",
       "   (42, 1),\n",
       "   (43, 3),\n",
       "   (44, 14),\n",
       "   (45, 2),\n",
       "   (46, 1),\n",
       "   (47, 2),\n",
       "   (48, 3),\n",
       "   (49, 1),\n",
       "   (50, 3),\n",
       "   (51, 1),\n",
       "   (52, 1),\n",
       "   (53, 2),\n",
       "   (55, 1),\n",
       "   (58, 1),\n",
       "   (61, 3),\n",
       "   (64, 1),\n",
       "   (67, 6),\n",
       "   (69, 1),\n",
       "   (70, 3),\n",
       "   (72, 3),\n",
       "   (73, 2),\n",
       "   (74, 1),\n",
       "   (75, 3),\n",
       "   (76, 1),\n",
       "   (80, 3),\n",
       "   (82, 1),\n",
       "   (84, 16),\n",
       "   (87, 1),\n",
       "   (89, 1),\n",
       "   (90, 1),\n",
       "   (97, 9),\n",
       "   (100, 3),\n",
       "   (101, 29),\n",
       "   (103, 2),\n",
       "   (104, 13),\n",
       "   (110, 1),\n",
       "   (111, 8),\n",
       "   (116, 2),\n",
       "   (118, 9),\n",
       "   (121, 1),\n",
       "   (123, 1),\n",
       "   (126, 1),\n",
       "   (127, 2),\n",
       "   (132, 4),\n",
       "   (135, 1),\n",
       "   (138, 1),\n",
       "   (142, 4),\n",
       "   (144, 1),\n",
       "   (145, 1),\n",
       "   (149, 2),\n",
       "   (151, 1),\n",
       "   (152, 5),\n",
       "   (153, 1),\n",
       "   (156, 2),\n",
       "   (157, 1),\n",
       "   (160, 1),\n",
       "   (161, 1),\n",
       "   (164, 2),\n",
       "   (166, 1),\n",
       "   (167, 1),\n",
       "   (175, 1),\n",
       "   (176, 1),\n",
       "   (182, 1),\n",
       "   (186, 1),\n",
       "   (187, 1),\n",
       "   (197, 3),\n",
       "   (200, 1),\n",
       "   (201, 1),\n",
       "   (203, 1),\n",
       "   (204, 1),\n",
       "   (205, 2),\n",
       "   (211, 6),\n",
       "   (213, 2),\n",
       "   (216, 1),\n",
       "   (224, 1),\n",
       "   (227, 2),\n",
       "   (228, 2),\n",
       "   (232, 2),\n",
       "   (233, 2),\n",
       "   (237, 2),\n",
       "   (240, 4),\n",
       "   (246, 2),\n",
       "   (249, 3),\n",
       "   (250, 2),\n",
       "   (251, 2),\n",
       "   (253, 1),\n",
       "   (260, 2),\n",
       "   (263, 1),\n",
       "   (266, 1),\n",
       "   (270, 1),\n",
       "   (273, 1),\n",
       "   (284, 1),\n",
       "   (286, 1),\n",
       "   (290, 1),\n",
       "   (291, 2),\n",
       "   (296, 2),\n",
       "   (299, 2),\n",
       "   (304, 1),\n",
       "   (307, 1),\n",
       "   (309, 2),\n",
       "   (328, 1),\n",
       "   (341, 2),\n",
       "   (344, 1),\n",
       "   (350, 1),\n",
       "   (353, 2),\n",
       "   (355, 1),\n",
       "   (363, 2),\n",
       "   (364, 1),\n",
       "   (365, 1),\n",
       "   (368, 1),\n",
       "   (373, 1),\n",
       "   (374, 1),\n",
       "   (375, 1),\n",
       "   (383, 1),\n",
       "   (384, 1),\n",
       "   (386, 1),\n",
       "   (389, 1),\n",
       "   (400, 1),\n",
       "   (412, 1),\n",
       "   (413, 1),\n",
       "   (414, 1),\n",
       "   (415, 1),\n",
       "   (416, 1),\n",
       "   (417, 2),\n",
       "   (418, 1),\n",
       "   (419, 1),\n",
       "   (420, 1),\n",
       "   (421, 1),\n",
       "   (422, 1),\n",
       "   (423, 1),\n",
       "   (424, 1),\n",
       "   (425, 1),\n",
       "   (426, 1),\n",
       "   (427, 1),\n",
       "   (428, 4),\n",
       "   (429, 1),\n",
       "   (430, 1),\n",
       "   (431, 1),\n",
       "   (432, 1),\n",
       "   (433, 1),\n",
       "   (434, 1),\n",
       "   (435, 1),\n",
       "   (436, 2),\n",
       "   (437, 1),\n",
       "   (438, 1),\n",
       "   (439, 1),\n",
       "   (440, 1)],\n",
       "  {'ATF': 1,\n",
       "   'Ajiduah': 1,\n",
       "   'Anthony': 1,\n",
       "   'Aubrey': 1,\n",
       "   'Batson': 1,\n",
       "   'Brit': 1,\n",
       "   'Cameron': 1,\n",
       "   'Chancler': 1,\n",
       "   'Dallas': 1,\n",
       "   'Encalade': 1,\n",
       "   'Featherston': 1,\n",
       "   'February': 1,\n",
       "   'Frisco': 2,\n",
       "   'Garrett': 2,\n",
       "   'Grindr': 1,\n",
       "   'Mody': 1,\n",
       "   'Nigel': 2,\n",
       "   'Plano': 2,\n",
       "   'Probation': 1,\n",
       "   'Saeed': 1,\n",
       "   'Shelton': 4,\n",
       "   'Tracey': 1,\n",
       "   'Upon': 1,\n",
       "   'arrange': 1,\n",
       "   'await': 1,\n",
       "   'brandished': 1,\n",
       "   'carjackings': 1,\n",
       "   'completion': 1,\n",
       "   'dating': 1,\n",
       "   'derogatory': 1,\n",
       "   'eighteen': 1,\n",
       "   'entering': 1,\n",
       "   'ethnicity': 1,\n",
       "   'factors': 1,\n",
       "   'firearm': 1,\n",
       "   'firearms': 1,\n",
       "   'gay': 2,\n",
       "   'gender': 1,\n",
       "   'identity': 1,\n",
       "   'invasion': 1,\n",
       "   'invasions': 1,\n",
       "   'investigative': 1,\n",
       "   'kidnappings': 1,\n",
       "   'leave': 1,\n",
       "   'media': 1,\n",
       "   'meet': 1,\n",
       "   'motor': 1,\n",
       "   'nationality': 1,\n",
       "   'orientation': 4,\n",
       "   'perpetrated': 1,\n",
       "   'physically': 1,\n",
       "   'platform': 1,\n",
       "   'presentence': 1,\n",
       "   'principles': 1,\n",
       "   'prohibited': 1,\n",
       "   'restrained': 1,\n",
       "   'stole': 1,\n",
       "   'stone': 1,\n",
       "   'subsequently': 1,\n",
       "   'superseding': 1,\n",
       "   'tape': 1,\n",
       "   'turned': 1,\n",
       "   'un': 1,\n",
       "   'values': 1,\n",
       "   'vehicle': 1,\n",
       "   'your': 1}),\n",
       " ([(0, 33),\n",
       "   (1, 10),\n",
       "   (2, 28),\n",
       "   (3, 5),\n",
       "   (8, 2),\n",
       "   (10, 2),\n",
       "   (11, 3),\n",
       "   (12, 3),\n",
       "   (15, 5),\n",
       "   (16, 3),\n",
       "   (17, 3),\n",
       "   (18, 1),\n",
       "   (21, 1),\n",
       "   (27, 3),\n",
       "   (29, 4),\n",
       "   (32, 6),\n",
       "   (33, 3),\n",
       "   (35, 2),\n",
       "   (36, 13),\n",
       "   (39, 2),\n",
       "   (41, 2),\n",
       "   (42, 1),\n",
       "   (43, 3),\n",
       "   (44, 20),\n",
       "   (45, 1),\n",
       "   (46, 4),\n",
       "   (47, 2),\n",
       "   (48, 1),\n",
       "   (50, 6),\n",
       "   (51, 4),\n",
       "   (55, 2),\n",
       "   (61, 2),\n",
       "   (63, 1),\n",
       "   (67, 3),\n",
       "   (69, 1),\n",
       "   (71, 1),\n",
       "   (72, 2),\n",
       "   (73, 14),\n",
       "   (78, 1),\n",
       "   (80, 1),\n",
       "   (81, 1),\n",
       "   (84, 23),\n",
       "   (89, 3),\n",
       "   (90, 1),\n",
       "   (96, 2),\n",
       "   (97, 7),\n",
       "   (100, 4),\n",
       "   (101, 41),\n",
       "   (102, 3),\n",
       "   (103, 2),\n",
       "   (104, 25),\n",
       "   (105, 1),\n",
       "   (106, 2),\n",
       "   (110, 2),\n",
       "   (111, 16),\n",
       "   (113, 6),\n",
       "   (115, 2),\n",
       "   (116, 3),\n",
       "   (118, 7),\n",
       "   (127, 4),\n",
       "   (135, 2),\n",
       "   (138, 1),\n",
       "   (145, 1),\n",
       "   (149, 1),\n",
       "   (152, 3),\n",
       "   (153, 2),\n",
       "   (155, 2),\n",
       "   (156, 3),\n",
       "   (162, 2),\n",
       "   (167, 2),\n",
       "   (170, 1),\n",
       "   (184, 1),\n",
       "   (186, 1),\n",
       "   (189, 2),\n",
       "   (190, 2),\n",
       "   (193, 1),\n",
       "   (195, 1),\n",
       "   (198, 4),\n",
       "   (199, 1),\n",
       "   (200, 1),\n",
       "   (203, 1),\n",
       "   (206, 1),\n",
       "   (211, 1),\n",
       "   (215, 1),\n",
       "   (220, 1),\n",
       "   (221, 3),\n",
       "   (226, 1),\n",
       "   (227, 2),\n",
       "   (228, 1),\n",
       "   (229, 1),\n",
       "   (230, 2),\n",
       "   (232, 2),\n",
       "   (234, 2),\n",
       "   (237, 2),\n",
       "   (238, 3),\n",
       "   (244, 1),\n",
       "   (246, 2),\n",
       "   (247, 1),\n",
       "   (248, 1),\n",
       "   (249, 1),\n",
       "   (254, 1),\n",
       "   (255, 1),\n",
       "   (256, 1),\n",
       "   (257, 1),\n",
       "   (261, 1),\n",
       "   (262, 1),\n",
       "   (265, 1),\n",
       "   (266, 1),\n",
       "   (269, 1),\n",
       "   (273, 1),\n",
       "   (284, 1),\n",
       "   (285, 1),\n",
       "   (291, 2),\n",
       "   (294, 1),\n",
       "   (298, 1),\n",
       "   (299, 1),\n",
       "   (302, 1),\n",
       "   (304, 1),\n",
       "   (307, 2),\n",
       "   (309, 2),\n",
       "   (311, 1),\n",
       "   (313, 1),\n",
       "   (317, 1),\n",
       "   (321, 1),\n",
       "   (328, 4),\n",
       "   (329, 3),\n",
       "   (345, 1),\n",
       "   (356, 1),\n",
       "   (363, 7),\n",
       "   (364, 2),\n",
       "   (365, 3),\n",
       "   (367, 6),\n",
       "   (368, 1),\n",
       "   (370, 1),\n",
       "   (373, 1),\n",
       "   (374, 1),\n",
       "   (375, 1),\n",
       "   (377, 1),\n",
       "   (379, 1),\n",
       "   (383, 1),\n",
       "   (384, 1),\n",
       "   (386, 1),\n",
       "   (392, 3),\n",
       "   (396, 1),\n",
       "   (404, 1),\n",
       "   (407, 1),\n",
       "   (417, 1),\n",
       "   (418, 6),\n",
       "   (422, 1),\n",
       "   (424, 1),\n",
       "   (428, 1),\n",
       "   (441, 1),\n",
       "   (442, 2),\n",
       "   (443, 2),\n",
       "   (444, 1),\n",
       "   (445, 1),\n",
       "   (446, 1),\n",
       "   (447, 1),\n",
       "   (448, 2),\n",
       "   (449, 1),\n",
       "   (450, 1),\n",
       "   (451, 2),\n",
       "   (452, 2),\n",
       "   (453, 1),\n",
       "   (454, 1),\n",
       "   (455, 1),\n",
       "   (456, 1),\n",
       "   (457, 1),\n",
       "   (458, 1),\n",
       "   (459, 2),\n",
       "   (460, 1),\n",
       "   (461, 1),\n",
       "   (462, 1),\n",
       "   (463, 1),\n",
       "   (464, 1),\n",
       "   (465, 1),\n",
       "   (466, 1),\n",
       "   (467, 1),\n",
       "   (468, 2),\n",
       "   (469, 1),\n",
       "   (470, 1),\n",
       "   (471, 3),\n",
       "   (472, 3),\n",
       "   (473, 1),\n",
       "   (474, 1)],\n",
       "  {'108': 1,\n",
       "   '156': 1,\n",
       "   '29': 1,\n",
       "   '36': 1,\n",
       "   '49': 1,\n",
       "   'Blue': 11,\n",
       "   'Bradley': 1,\n",
       "   'Bryan': 1,\n",
       "   'Carolina': 3,\n",
       "   'Carolinians': 1,\n",
       "   'Caucasian': 3,\n",
       "   'Dee': 1,\n",
       "   'Feldner': 1,\n",
       "   'Florence': 1,\n",
       "   'Frank': 1,\n",
       "   'Great': 1,\n",
       "   'Hartley': 1,\n",
       "   'Harwell': 2,\n",
       "   'Howard': 3,\n",
       "   'Investigator': 1,\n",
       "   'Jeffrey': 1,\n",
       "   'Judson': 1,\n",
       "   'Kevin': 1,\n",
       "   'Key': 1,\n",
       "   'Landing': 1,\n",
       "   'Marlboro': 1,\n",
       "   'McDonald': 1,\n",
       "   'Once': 1,\n",
       "   'Parham': 1,\n",
       "   'Pee': 1,\n",
       "   'River': 1,\n",
       "   'Shawn': 1,\n",
       "   'Shop': 3,\n",
       "   'South': 4,\n",
       "   'Sr': 2,\n",
       "   'Stokes': 1,\n",
       "   'Stop': 3,\n",
       "   'Talbert': 2,\n",
       "   'actually': 2,\n",
       "   'aiding': 1,\n",
       "   'anything': 1,\n",
       "   'arrived': 1,\n",
       "   'assaults': 1,\n",
       "   'attacked': 2,\n",
       "   'bigotry': 1,\n",
       "   'car': 3,\n",
       "   'carjack': 1,\n",
       "   'carjacking': 2,\n",
       "   'chainsaw': 1,\n",
       "   'conspirator': 1,\n",
       "   'crowd': 1,\n",
       "   'denied': 1,\n",
       "   'deprive': 1,\n",
       "   'depriving': 2,\n",
       "   'elder': 5,\n",
       "   'escaped': 1,\n",
       "   'escorted': 1,\n",
       "   'establishment': 1,\n",
       "   'even': 1,\n",
       "   'expect': 1,\n",
       "   'facilities': 1,\n",
       "   'federally': 2,\n",
       "   'feel': 1,\n",
       "   'firearm': 1,\n",
       "   'forced': 1,\n",
       "   'forcibly': 1,\n",
       "   'frequently': 1,\n",
       "   'fueled': 3,\n",
       "   'happen': 1,\n",
       "   'kill': 2,\n",
       "   'knowing': 1,\n",
       "   'lot': 1,\n",
       "   'nearby': 1,\n",
       "   'note': 1,\n",
       "   'parking': 1,\n",
       "   'pistol': 3,\n",
       "   'pointed': 1,\n",
       "   'product': 1,\n",
       "   'pursued': 1,\n",
       "   'quite': 1,\n",
       "   'racism': 1,\n",
       "   'ready': 1,\n",
       "   'realizing': 1,\n",
       "   'refuge': 1,\n",
       "   'regardless': 1,\n",
       "   'relation': 1,\n",
       "   'reminder': 1,\n",
       "   'restroom': 1,\n",
       "   'retrieve': 1,\n",
       "   'retrieved': 1,\n",
       "   'road': 1,\n",
       "   'second': 1,\n",
       "   'seeking': 1,\n",
       "   'senseless': 1,\n",
       "   'sentences': 1,\n",
       "   'site': 1,\n",
       "   'small': 1,\n",
       "   'stand': 1,\n",
       "   'stole': 1,\n",
       "   'store': 1,\n",
       "   'stuck': 1,\n",
       "   'tempted': 1,\n",
       "   'terrifying': 1,\n",
       "   'terrorizing': 1,\n",
       "   'third': 3,\n",
       "   'too': 1,\n",
       "   'watched': 1,\n",
       "   'whose': 1}),\n",
       " ([(0, 1),\n",
       "   (2, 3),\n",
       "   (11, 1),\n",
       "   (15, 1),\n",
       "   (32, 1),\n",
       "   (39, 1),\n",
       "   (44, 2),\n",
       "   (84, 1),\n",
       "   (97, 1),\n",
       "   (99, 1),\n",
       "   (100, 1),\n",
       "   (101, 7),\n",
       "   (104, 1),\n",
       "   (105, 1),\n",
       "   (118, 1),\n",
       "   (127, 1),\n",
       "   (149, 1),\n",
       "   (161, 1),\n",
       "   (167, 1),\n",
       "   (176, 1),\n",
       "   (184, 1),\n",
       "   (186, 1),\n",
       "   (212, 1),\n",
       "   (249, 1),\n",
       "   (250, 1),\n",
       "   (351, 1),\n",
       "   (377, 1),\n",
       "   (383, 1),\n",
       "   (386, 1),\n",
       "   (395, 1),\n",
       "   (475, 1),\n",
       "   (476, 1),\n",
       "   (477, 1),\n",
       "   (478, 1),\n",
       "   (479, 1),\n",
       "   (480, 1)],\n",
       "  {'Dylann': 1,\n",
       "   'Following': 1,\n",
       "   'Loretta': 1,\n",
       "   'Lynch': 1,\n",
       "   'Roof': 1,\n",
       "   'compelled': 1,\n",
       "   'consider': 1,\n",
       "   'death': 1,\n",
       "   'decision': 1,\n",
       "   'determined': 1,\n",
       "   'factual': 1,\n",
       "   'issues': 1,\n",
       "   'legal': 1,\n",
       "   'nature': 1,\n",
       "   'released': 1,\n",
       "   'relevant': 1,\n",
       "   'resulting': 1,\n",
       "   'review': 1,\n",
       "   'rigorous': 1,\n",
       "   'thoroughly': 1,\n",
       "   'v': 1})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 1: re-tokenize and store in list\n",
    "## here, i'm doing with the raw random sample of text\n",
    "## in activity, you should do with the preprocessed texts\n",
    "\n",
    "text_raw_tokens = [wordpunct_tokenize(one_text) for one_text in \n",
    "                  doj_subset_wscore.contents]\n",
    "\n",
    "\n",
    "## Step 2: use gensim create dictionary - gets all unique words across documents\n",
    "text_raw_dict = corpora.Dictionary(text_raw_tokens)\n",
    "raw_len = len(text_raw_dict) # get length for comparison below\n",
    "\n",
    "### explore first few keys and values\n",
    "### see that key is just an arbitrary counter; value is the word itself\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]}\n",
    "\n",
    "\n",
    "## Step 3: filter out very rare and very common words\n",
    "## here, i'm using the threshold that a word needs to appear in at least\n",
    "## 5% of docs but not more than 95%\n",
    "## this is an integer count of docs so i round\n",
    "lower_bound = round(doj_subset_wscore.shape[0]*0.05)\n",
    "upper_bound = round(doj_subset_wscore.shape[0]*0.95)\n",
    "\n",
    "### apply filtering to dictionary\n",
    "text_raw_dict.filter_extremes(no_below = lower_bound,\n",
    "                             no_above = upper_bound)\n",
    "print(f'Filtering out very rare and very common words reduced the \\\n",
    "length of dictionary from {str(raw_len)} to {str(len(text_raw_dict))}.')\n",
    "{k: text_raw_dict[k] for k in list(text_raw_dict)[:5]} # show first five entries after filtering\n",
    "\n",
    "\n",
    "## Step 4: apply dictionary to TOKENIZED texts\n",
    "## this creates a mapping between each word \n",
    "## in a specific listing and the key in the dictionary.\n",
    "## for words that remain in the filtered dictionary,\n",
    "## output is a list where len(list) == n documents\n",
    "## and each element in the list is a list of tuples\n",
    "## containing the mappings\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### can apply doc2bow(one_text, return_missing = True) to print words\n",
    "### eliminated from the listing bc they're not in filtered dictionary.\n",
    "### but feeding that one with missing values to\n",
    "### the lda function can cause errors\n",
    "corpus_fromdict_showmiss = [text_raw_dict.doc2bow(one_text, return_missing = True)\n",
    "                            for one_text in text_raw_tokens]\n",
    "print('Sample of documents represented in dictionary format (with omitted words noted):')\n",
    "corpus_fromdict_showmiss[:10]\n",
    "\n",
    "ldamod = gensim.models.ldamodel.LdaModel(corpus_fromdict, \n",
    "                                         num_topics = 3, \n",
    "                                         id2word=text_raw_dict, \n",
    "                                         passes=6, \n",
    "                                         alpha = 'auto',\n",
    "                                         per_word_topics = True, \n",
    "                                         random_state = 91988)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.071*\".\" + 0.060*\",\" + 0.052*\"of\" + 0.051*\"the\" + 0.040*\"and\" + 0.030*\"to\" + 0.021*\"in\" + 0.016*\"a\" + 0.014*\"’\" + 0.012*\"s\" + 0.011*\"S\" + 0.010*\"U\" + 0.010*\"Attorney\" + 0.009*\"by\" + 0.009*\"was\"')\n",
      "\n",
      "(1, '0.063*\"the\" + 0.063*\".\" + 0.062*\",\" + 0.045*\"of\" + 0.036*\"and\" + 0.029*\"to\" + 0.022*\"in\" + 0.022*\"a\" + 0.011*\"’\" + 0.011*\"s\" + 0.010*\"Attorney\" + 0.010*\"for\" + 0.010*\"The\" + 0.010*\"that\" + 0.009*\"by\"')\n",
      "\n",
      "(2, '0.068*\"the\" + 0.050*\".\" + 0.048*\",\" + 0.037*\"of\" + 0.035*\"and\" + 0.033*\"to\" + 0.019*\"in\" + 0.017*\"a\" + 0.016*\"that\" + 0.016*\"’\" + 0.016*\"-\" + 0.015*\"The\" + 0.015*\"s\" + 0.013*\"with\" + 0.011*\"for\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## B: print the top 15 words in each topic \n",
    "\n",
    "topics = ldamod.print_topics(num_words = 15)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Add topics back to main data and explore correlation between manual labels and our estimated topics (10 points)\n",
    "\n",
    "A. Extract the document-level topic probabilities. Within `get_document_topics`, use the argument `minimum_probability` = 0 to make sure all 3 topic probabilities are returned. Write an assert statement to make sure the length of the list is equal to the number of rows in the `doj_subset_wscores` dataframe\n",
    "\n",
    "B. Add the topic probabilities to the `doj_subset_wscores` dataframe as columns and create a column, `top_topic`, that reflects each document to its highest-probability topic (eg topic 1, 2, or 3)\n",
    "\n",
    "C. For each of the manual labels in `topics_clean` (Hate Crime, Civil Rights, Project Safe Childhood), print the breakdown of the % of documents with each top topic (so, for instance, Hate Crime has 246 documents-- if 123 of those documents are coded to topic_1, that would be 50%; and so on). **Hint**: pd.crosstab and normalize may be helpful: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.crosstab.html\n",
    "\n",
    "D. Using a couple press releases as examples, write a 1-2 sentence interpretation of why some of the manual topics map on more cleanly to an estimated topic than other manual topic(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.00036581067), (1, 0.99931455), (2, 0.00031964845)],\n",
       " [(0, 0.0004575853), (1, 0.9991425), (2, 0.00039984199)],\n",
       " [(0, 0.0002844792), (1, 0.99946696), (2, 0.00024857657)],\n",
       " [(0, 0.00025112883), (1, 0.9995295), (2, 0.0002194352)],\n",
       " [(0, 0.00034774918), (1, 0.9993484), (2, 0.0003038668)]]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A: code to get doc-level topic probabilities \n",
    "\n",
    "corpus_fromdict = [text_raw_dict.doc2bow(one_text) \n",
    "                   for one_text in text_raw_tokens]\n",
    "\n",
    "### for each item in our original dictionary, get list of topic probabilities\n",
    "l = [ldamod.get_document_topics(topic, minimum_probability = 0) for topic in corpus_fromdict]\n",
    "\n",
    "### print result\n",
    "l[0:5]\n",
    "\n",
    "### write an assert statement \n",
    "\n",
    "assert len(l) == len(doj_subset_wscore), \"Length of the list, l, and the dataframe, doj_subset_wscore are not equal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B: add those topic probabilities to the dataframe\n",
    "\n",
    "assert len(l) == len(doj_subset_wscore), \"Length of l not equal to length of doj_subset_wscore\"\n",
    "\n",
    "document_topic_probabilities = []\n",
    "\n",
    "for topic in corpus_fromdict:\n",
    "    topic_probabilities = ldamod.get_document_topics(topic, minimum_probability = 0)\n",
    "\n",
    "document_topic_probabilities.append([topic for _, topic in topic_probabilities])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'top_topic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'top_topic'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## C: summarize the topic proportions for each of the topics_clean \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m percent_top \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcrosstab(doj_subset_wscore[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_clean\u001b[39m\u001b[38;5;124m'\u001b[39m], doj_subset_wscore[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_topic\u001b[39m\u001b[38;5;124m'\u001b[39m], normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(percent_top)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'top_topic'"
     ]
    }
   ],
   "source": [
    "## C: summarize the topic proportions for each of the topics_clean \n",
    "\n",
    "percent_top = pd.crosstab(doj_subset_wscore['topics_clean'], doj_subset_wscore['top_topic'], normalize='index') * 100\n",
    "\n",
    "print(percent_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## D: why some of the manual topics map on more cleanly to an estimated topic than other manual topic(s)\n",
    "\n",
    "example_1 = doj_subset_wscore[doj_subset_wscore['id'] == '14-248']\n",
    "\n",
    "example_2 = doj_subset_wscore[doj_subset_wscore['id'] == '13-312']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extend the analysis from unigrams to bigrams (10 points)\n",
    "\n",
    "In the previous question, you found top words via a unigram representation of the text. Now, we want to see how those top words change with bigrams (pairs of words)\n",
    "\n",
    "A. Using the `doj_subset_wscore` data and the `processed_text` column (so the words after stemming/other preprocessing), create a column in the data called `processed_text_bigrams` that combines each consecutive pairs of word into a bigram separated by an underscore. Eg:\n",
    "\n",
    "\"depart reach settlem\" would become \"depart_reach reach_settlem\"\n",
    "\n",
    "Do this by writing a function `create_bigram_onedoc` that takes in a single `processed_text` string and returns a string with its bigrams structured similarly to above example\n",
    " \n",
    "**Hint**: there are many ways to solve but `zip` may be helpful: https://stackoverflow.com/questions/21303224/iterate-over-all-pairs-of-consecutive-items-in-a-list\n",
    "\n",
    "B. Print the `id`, `processed_text`, and `processed_text_bigram` columns for press release with id = 16-217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## A: creaing a new column — processed_texts_bigrams \n",
    "\n",
    "def create_bigram_onedoc(text):\n",
    "\n",
    "    words = text.split()\n",
    "    \n",
    "    bigrams = ['_'.join(bigram) for bigram in zip(words, words[1:])]\n",
    "    \n",
    "    return \" \".join(bigrams)\n",
    "\n",
    "doj_subset_wscore['processed_text_bigrams'] = [create_bigram_onedoc(processed_content) for processed_content in doj_subset_wscore['processed_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B: printing columns for press release with id = 16-217\n",
    "\n",
    "rowz_16217 = doj_subset_wscore[doj_subset_wscore['id'] == '16-217']\n",
    "\n",
    "rowz_16217[['id', 'processed_text', 'processed_text_bigrams']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use the create_dtm function and the `processed_text_bigrams` column to create a document-term matrix (`dtm_bigram`) with these bigrams. Keep the following three columns in the data: `id`, `topics_clean`, and `compound` \n",
    "\n",
    "D. Print the (1) dimensions of the `dtm` matrix from question 2.2  and (2) the dimensions of the `dtm_bigram` matrix. Comment on why the bigram matrix has more dimensions than the unigram matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Find and print the 10 most prevelant bigrams for each of the three topics_clean using the `get_topwords` function from 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C: creating dtm_bigram \n",
    "\n",
    "dtm_bigram = create_dtm(list_of_strings = doj_subset_wscore['processed_text_bigrams'],\n",
    "                metadata = doj_subset_wscore[['id', 'topics_clean', 'compound_score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(717, 6905)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dtm_bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[357], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## D: printing the dimensions of dtm matrix and dtm_bigram matrix \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dtm_nopre\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m dtm_bigram\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm_bigram' is not defined"
     ]
    }
   ],
   "source": [
    "## D: printing the dimensions of dtm matrix and dtm_bigram matrix \n",
    "\n",
    "print(dtm_nopre.shape)\n",
    "\n",
    "dtm_bigram.shape\n",
    "\n",
    "## Comment on why the bigram matrix has more dimensions than the unigram matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E: Find and print the 10 most prevelant bigrams for each of the three topics_clean using the get_topwords function from 2.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional extra credit (2 points)\n",
    "\n",
    "You notice that the pharmaceutical kickbacks press release we analyzed in question 1 was for an indictment, and that in the original data, there's not a clear label for whether a press release outlines an indictment (charging someone with a crime), a conviction (convicting them after that charge either via a settlement or trial), or a sentencing (how many years of prison or supervised release a defendant is sentenced to after their conviction).\n",
    "\n",
    "You want to see if you can identify pairs of press releases where one press release is from one stage (e.g., indictment) and another is from a different stage (e.g., a sentencing).\n",
    "\n",
    "You decide that one way to approach is to find the pairwise string similarity between each of the processed press releases in `doj_subset`. There are many ways to do this, so Google for some approaches, focusing on ones that work well for entire documents rather than small strings.\n",
    "\n",
    "Find the top two pairs (so four press releases total)-- do they seem like different stages of the same crime or just press releases covering similar crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## uncomment and download if this is your first \n",
    "## time running \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## specify to print all output in a call\n",
    "## and not just first\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy --- if you get an error at the load step\n",
    "## need to download en_core_web_sm (google)\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  \\\n",
       "0  2539                Clean & quiet apt home by the park   \n",
       "1  2595                             Skylit Midtown Castle   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
       "3  3831                   Cozy Entire Floor of Brownstone   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
       "\n",
       "                                         name_upper neighbourhood_group  price  \n",
       "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
       "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
       "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
       "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
       "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48895 entries, 0 to 48894\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   48895 non-null  int64 \n",
      " 1   name                 48879 non-null  object\n",
      " 2   name_upper           48879 non-null  object\n",
      " 3   neighbourhood_group  48895 non-null  object\n",
      " 4   price                48895 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "## if working from within the repo, can use this relative path\n",
    "path_todata = \"../public_data/airbnb_text.csv\"\n",
    "\n",
    "## load data\n",
    "ab = pd.read_csv(path_todata)\n",
    "ab.head()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 1: look for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_mention_cozy</th>\n",
       "      <th>mention_cozy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bronx</th>\n",
       "      <td>89.231088</td>\n",
       "      <td>74.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn</th>\n",
       "      <td>128.175441</td>\n",
       "      <td>91.130224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manhattan</th>\n",
       "      <td>204.109775</td>\n",
       "      <td>129.917140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queens</th>\n",
       "      <td>102.596682</td>\n",
       "      <td>80.344388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staten Island</th>\n",
       "      <td>120.650307</td>\n",
       "      <td>74.319149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     no_mention_cozy  mention_cozy\n",
       "neighbourhood_group                               \n",
       "Bronx                      89.231088     74.214286\n",
       "Brooklyn                  128.175441     91.130224\n",
       "Manhattan                 204.109775    129.917140\n",
       "Queens                    102.596682     80.344388\n",
       "Staten Island             120.650307     74.319149"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the `name_upper` var, look at where reviews mention cozy\n",
    "ab['is_cozy'] = np.where(ab.name_upper.str.contains(\"COZY\"), True, False)\n",
    "\n",
    "## find the mean price by neighborhood and whether cozy\n",
    "mp = pd.DataFrame(ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean())\n",
    "\n",
    "## reshape to wide format so that each borough is row\n",
    "## and one col is the mean price for listings that describe\n",
    "## the place as cozy; other col is mean price for listings\n",
    "## without that word\n",
    "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
    "                        columns = ['is_cozy'])\n",
    "\n",
    "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
    "\n",
    "mp_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 2: score based on dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COZY', 'LITTLE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## construct dictionary\n",
    "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
    "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
    "\n",
    "\n",
    "## for each listing, find the number of occurrences\n",
    "## of words in each key\n",
    "\n",
    "### first, let's test with one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "\n",
    "### splitting that string at space and looking at overlap with each key\n",
    "### first, look at overlap with the list containing words for small\n",
    "words_overlap_small = [word \n",
    "                    for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['small']]\n",
    "words_overlap_small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### then, look at overlap with the list containing words for large\n",
    "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['large']]\n",
    "words_overlap_large\n",
    "\n",
    "### could then take length as a fraction of all words\n",
    "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
    "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a chill apt next to the subway in LES Chinatown'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## specify example\n",
    "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
    "example_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/maryford/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## try part of speech tagging using nltk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(example_for_tag) \u001b[38;5;66;03m# Generate list of tokens\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tokens_pos \u001b[38;5;241m=\u001b[39m pos_tag(tokens) \u001b[38;5;66;03m# generate part of speech tags for those tokens\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## returns a list of tuples\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m## first element in tuple is a word\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## second element in tuple is the part of speech\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#for one_tok in tokens_pos:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m  \u001b[38;5;66;03m#   print(one_tok)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/maryford/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "## try part of speech tagging using nltk\n",
    "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
    " \n",
    "## returns a list of tuples\n",
    "## first element in tuple is a word\n",
    "## second element in tuple is the part of speech\n",
    "#for one_tok in tokens_pos:\n",
    " #   print(one_tok)\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## use list iteration to extract proper nouns (NNP)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## i'm first checking if the second element in the tuple\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## is equal to NNP\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## if so, i'm returning the first element in the tuple (the \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## actual word)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m all_prop_noun \u001b[38;5;241m=\u001b[39m [one_tok[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m one_tok \u001b[38;5;129;01min\u001b[39;00m tokens_pos \n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m all_prop_noun\n\u001b[1;32m     10\u001b[0m all_adj_noun \u001b[38;5;241m=\u001b[39m [one_tok[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m one_tok \u001b[38;5;129;01min\u001b[39;00m tokens_pos \n\u001b[1;32m     11\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[1;32m     12\u001b[0m                one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_pos' is not defined"
     ]
    }
   ],
   "source": [
    "## use list iteration to extract proper nouns (NNP)\n",
    "## i'm first checking if the second element in the tuple\n",
    "## is equal to NNP\n",
    "## if so, i'm returning the first element in the tuple (the \n",
    "## actual word)\n",
    "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"NNP\"]\n",
    "all_prop_noun\n",
    "\n",
    "all_adj_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"JJ\" or \n",
    "               one_tok[1] == \"NN\"]\n",
    "all_adj_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified from a real tweet\n",
    "\n",
    "## tweet\n",
    "d_tweet = \"\"\"We’ll be hosting on-campus COVID-19 booster clinics \n",
    "at Dartmouth College in New Hampshire from 9 a.m. to 6 p.m. on\n",
    "Monday, Jan. 10, and Tuesday, Jan. 11, at\n",
    "Alumni Hall in the Hopkins Center. For information on how to\n",
    "register and additional winter updates, head to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dtweet = nlp(d_tweet)\n",
    "print(type(spacy_dtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## try a couple variations\n",
    "for one_tok in spacy_dtweet.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Play around with different variations of the Dartmouth tweet and look at the results. For instance, try the following:\n",
    "\n",
    "- What happens if you abbreviate New Hampshire to NH?\n",
    "- What happens if you add the word Pfizer before COVID-19?\n",
    "- What entities seem misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "How do we generalize from doing Named Entity Recognition one just one string to doing NER on a whole column that contains strings? By defining and executing a **function**, of course!\n",
    "\n",
    "Using the following sample of strings from airbnb listing names, define a function that takes in one airbnb string and does the following:\n",
    "\n",
    "- iterates over entities in that string (list comprehension would work well for this)\n",
    "- checks if each label indicates a place\n",
    "- returns the text of each original place/entity\n",
    "\n",
    "Then execute the function on the example strings through iteration (again, list comprehension works well here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for runtime purposes, take a sample of the airbnb listing names\n",
    "ab_name_examples = ab.name[10:15]\n",
    "ab_name_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default scorer on a few example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase with word terrible and score\n",
    "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
    "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase about rats; bad but might not be in scoring dictionary\n",
    "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
    "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize all 3\n",
    "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
    "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
    "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the dictionary with manually-added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sent_obj.lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lexicon is a dictionary where the key\n",
    "## is the word\n",
    "## the value is the score (negative = negative)\n",
    "## here, i'm benchmarking the negativity of the\n",
    "## rodents to the negativity of the word aversion\n",
    "sent_obj.lexicon['aversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary with \n",
    "## negative scores for pests\n",
    "pest_words = {\n",
    "    'rat': -1.9,\n",
    "    'rats': -1.9,\n",
    "    'mice': -1.9,\n",
    "    'mouse': -1.9,\n",
    "    'roach': -1.9,\n",
    "    'cockroach': -1.9\n",
    "}\n",
    "\n",
    "\n",
    "## initiate new sentiment object\n",
    "## so that we don't alter old one\n",
    "## use.update to add new words\n",
    "new_si = SentimentIntensityAnalyzer()\n",
    "new_si.lexicon.update(pest_words)\n",
    "\n",
    "## try re-scoring the third example\n",
    "## see negative\n",
    "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
    "      str(new_si.polarity_scores(practice_listing_3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
